{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Processing with Disaster Tweets\n",
    "##### Authors: Ruth Ashford, Toby Petty and Li Jin\n",
    "This report is based on a Kaggle competition: https://www.kaggle.com/c/nlp-getting-started/data\n",
    "\n",
    "The aim is to classify tweets as being about a disaster or not. We are given a labelled training data set a long with a unlabelled test data set. \n",
    "\n",
    "In the report, we perform initial EDA on the data, cleaning and processing of the tweets text and evaluate the performance Naive Bayes, Neural Network and Logistic Regression models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (7613, 5)\n",
      "Test:  (3263, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation orders in California</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location  \\\n",
       "0   1     NaN      NaN   \n",
       "1   4     NaN      NaN   \n",
       "2   5     NaN      NaN   \n",
       "3   6     NaN      NaN   \n",
       "4   7     NaN      NaN   \n",
       "\n",
       "                                                                                                                                    text  \\\n",
       "0                                                                  Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all   \n",
       "1                                                                                                 Forest fire near La Ronge Sask. Canada   \n",
       "2  All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected   \n",
       "3                                                                      13,000 people receive #wildfires evacuation orders in California    \n",
       "4                                               Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school    \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_colwidth\", 500)\n",
    "\n",
    "data_fp = os.path.join(os.getcwd(), \"data\")\n",
    "train_fp = os.path.join(data_fp, \"train.csv\")\n",
    "train_full = pd.read_csv(train_fp, encoding=\"utf-8\")\n",
    "test_fp = os.path.join(data_fp, \"test.csv\")\n",
    "test = pd.read_csv(test_fp, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"Train: {train_full.shape}\")\n",
    "print(f\"Test:  {test.shape}\")\n",
    "\n",
    "train_full.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kaggle supplies the test set (without target labels), so we will submit our final predictions there to get the ultimate measure of model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory data analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly we will check the number of samples that are labelled as disaster (1) and non-disaster (0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total</th>\n",
       "      <th>Percentage</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4342</td>\n",
       "      <td>57.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3271</td>\n",
       "      <td>43.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Total  Percentage\n",
       "target                   \n",
       "0        4342        57.0\n",
       "1        3271        43.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label_summary = train_full.groupby('target').agg(Total = ('id','count'))\n",
    "train_label_summary['Percentage'] = round(train_label_summary['Total'] / train_full.shape[0] * 100, 1)\n",
    "train_label_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows us that we have a fairly even split of disaster and non-disaster labelled samples. This is useful when training our model but tell us that this data set is not likely to be representative of the general population of tweets, we know in general there are far less than 43% of tweets that are about disasters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keywords\n",
    "Next we want to take a look at the keywords and how they relate to the target. The majority of tweets in our training data have a keyword associated with them, there is just 0.2% of samples that don't."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Percentage of tweets without a keyword: 0.2%\n"
     ]
    }
   ],
   "source": [
    "keyword_label_summary = train_full.groupby('keyword').agg(percentage_disaster_occurance = ('target', 'mean')).reset_index()\n",
    "\n",
    "keyword_disaster_count = train_full[train_full['target'] == 1].groupby('keyword').agg(disaster_count = ('target', 'count')).reset_index()\n",
    "keyword_not_disaster_count = train_full[train_full['target'] == 0].groupby('keyword').agg(not_disaster_count = ('target', 'count')).reset_index()\n",
    "\n",
    "keyword_label_summary = keyword_label_summary.merge(keyword_disaster_count, how='left')\n",
    "keyword_label_summary = keyword_label_summary.merge(keyword_not_disaster_count, how='left')\n",
    "\n",
    "print(f' Percentage of tweets without a keyword: {round(1 - train_full.keyword.isna().sum() / train_full.shape[0] * 100 , 2)}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the words that come up most often in disaster tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>percentage_disaster_occurance</th>\n",
       "      <th>disaster_count</th>\n",
       "      <th>not_disaster_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>debris</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>37.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>wreckage</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>39.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>derailment</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>39.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>outbreak</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>39.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>oil%20spill</td>\n",
       "      <td>0.973684</td>\n",
       "      <td>37.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>typhoon</td>\n",
       "      <td>0.973684</td>\n",
       "      <td>37.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>suicide%20bombing</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>suicide%20bomber</td>\n",
       "      <td>0.967742</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>bombing</td>\n",
       "      <td>0.931034</td>\n",
       "      <td>27.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>rescuers</td>\n",
       "      <td>0.914286</td>\n",
       "      <td>32.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>suicide%20bomb</td>\n",
       "      <td>0.914286</td>\n",
       "      <td>32.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>nuclear%20disaster</td>\n",
       "      <td>0.911765</td>\n",
       "      <td>31.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>evacuated</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>32.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>razed</td>\n",
       "      <td>0.885714</td>\n",
       "      <td>31.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>wildfire</td>\n",
       "      <td>0.878788</td>\n",
       "      <td>29.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>wild%20fires</td>\n",
       "      <td>0.870968</td>\n",
       "      <td>27.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>airplane%20accident</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>30.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>mass%20murder</td>\n",
       "      <td>0.848485</td>\n",
       "      <td>28.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>buildings%20on%20fire</td>\n",
       "      <td>0.848485</td>\n",
       "      <td>28.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>forest%20fires</td>\n",
       "      <td>0.843750</td>\n",
       "      <td>27.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   keyword  percentage_disaster_occurance  disaster_count  \\\n",
       "62                  debris                       1.000000            37.0   \n",
       "219               wreckage                       1.000000            39.0   \n",
       "70              derailment                       1.000000            39.0   \n",
       "153               outbreak                       0.975000            39.0   \n",
       "152            oil%20spill                       0.973684            37.0   \n",
       "205                typhoon                       0.973684            37.0   \n",
       "187      suicide%20bombing                       0.969697            32.0   \n",
       "186       suicide%20bomber                       0.967742            30.0   \n",
       "32                 bombing                       0.931034            27.0   \n",
       "166               rescuers                       0.914286            32.0   \n",
       "185         suicide%20bomb                       0.914286            32.0   \n",
       "147     nuclear%20disaster                       0.911765            31.0   \n",
       "96               evacuated                       0.888889            32.0   \n",
       "162                  razed                       0.885714            31.0   \n",
       "214               wildfire                       0.878788            29.0   \n",
       "213           wild%20fires                       0.870968            27.0   \n",
       "3      airplane%20accident                       0.857143            30.0   \n",
       "139          mass%20murder                       0.848485            28.0   \n",
       "35   buildings%20on%20fire                       0.848485            28.0   \n",
       "116         forest%20fires                       0.843750            27.0   \n",
       "\n",
       "     not_disaster_count  \n",
       "62                  NaN  \n",
       "219                 NaN  \n",
       "70                  NaN  \n",
       "153                 1.0  \n",
       "152                 1.0  \n",
       "205                 1.0  \n",
       "187                 1.0  \n",
       "186                 1.0  \n",
       "32                  2.0  \n",
       "166                 3.0  \n",
       "185                 3.0  \n",
       "147                 3.0  \n",
       "96                  4.0  \n",
       "162                 4.0  \n",
       "214                 4.0  \n",
       "213                 4.0  \n",
       "3                   5.0  \n",
       "139                 5.0  \n",
       "35                  5.0  \n",
       "116                 5.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keyword_label_summary.sort_values('percentage_disaster_occurance', \n",
    "                                  ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the kewords that come up most often in non-disaster tweets. I can see that 'body%20bags' and body%20bag' is a duplicate and needs cleaning up. There are a lot of words here that I would associate with disasters, such as 'ruin', 'explode' and 'wrecked' but these are also words that could be used in a sentence when not describing a disaster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>percentage_disaster_occurance</th>\n",
       "      <th>disaster_count</th>\n",
       "      <th>not_disaster_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aftershock</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>body%20bags</td>\n",
       "      <td>0.024390</td>\n",
       "      <td>1.0</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>ruin</td>\n",
       "      <td>0.027027</td>\n",
       "      <td>1.0</td>\n",
       "      <td>36.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>blazing</td>\n",
       "      <td>0.029412</td>\n",
       "      <td>1.0</td>\n",
       "      <td>33.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>body%20bag</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>1.0</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>electrocute</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>1.0</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>screaming</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>2.0</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>traumatised</td>\n",
       "      <td>0.057143</td>\n",
       "      <td>2.0</td>\n",
       "      <td>33.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>panicking</td>\n",
       "      <td>0.060606</td>\n",
       "      <td>2.0</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>blew%20up</td>\n",
       "      <td>0.060606</td>\n",
       "      <td>2.0</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>blight</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>2.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>wrecked</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>3.0</td>\n",
       "      <td>36.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>explode</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>3.0</td>\n",
       "      <td>35.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>panic</td>\n",
       "      <td>0.081081</td>\n",
       "      <td>3.0</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>epicentre</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>bloody</td>\n",
       "      <td>0.085714</td>\n",
       "      <td>3.0</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>smoke</td>\n",
       "      <td>0.088235</td>\n",
       "      <td>3.0</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>collide</td>\n",
       "      <td>0.088235</td>\n",
       "      <td>3.0</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>stretcher</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>3.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>drown</td>\n",
       "      <td>0.093750</td>\n",
       "      <td>3.0</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         keyword  percentage_disaster_occurance  disaster_count  \\\n",
       "2     aftershock                       0.000000             NaN   \n",
       "29   body%20bags                       0.024390             1.0   \n",
       "170         ruin                       0.027027             1.0   \n",
       "19       blazing                       0.029412             1.0   \n",
       "27    body%20bag                       0.030303             1.0   \n",
       "88   electrocute                       0.031250             1.0   \n",
       "173    screaming                       0.055556             2.0   \n",
       "201  traumatised                       0.057143             2.0   \n",
       "156    panicking                       0.060606             2.0   \n",
       "21     blew%20up                       0.060606             2.0   \n",
       "22        blight                       0.062500             2.0   \n",
       "220      wrecked                       0.076923             3.0   \n",
       "98       explode                       0.078947             3.0   \n",
       "155        panic                       0.081081             3.0   \n",
       "94     epicentre                       0.083333             1.0   \n",
       "25        bloody                       0.085714             3.0   \n",
       "180        smoke                       0.088235             3.0   \n",
       "48       collide                       0.088235             3.0   \n",
       "183    stretcher                       0.090909             3.0   \n",
       "83         drown                       0.093750             3.0   \n",
       "\n",
       "     not_disaster_count  \n",
       "2                  34.0  \n",
       "29                 40.0  \n",
       "170                36.0  \n",
       "19                 33.0  \n",
       "27                 32.0  \n",
       "88                 31.0  \n",
       "173                34.0  \n",
       "201                33.0  \n",
       "156                31.0  \n",
       "21                 31.0  \n",
       "22                 30.0  \n",
       "220                36.0  \n",
       "98                 35.0  \n",
       "155                34.0  \n",
       "94                 11.0  \n",
       "25                 32.0  \n",
       "180                31.0  \n",
       "48                 31.0  \n",
       "183                30.0  \n",
       "83                 29.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keyword_label_summary.sort_values('percentage_disaster_occurance', \n",
    "                                  ascending=True).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Length of tweets\n",
    "We want to understand if the length of the tweet differs for disaster tweets versus non-disaster tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length of disaster tweets is 108 with standard deviation 29\n",
      "Average length of non-disaster tweets is 96 with standard deviation 36\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(2)\n",
    "fig. tight_layout(pad=3.0)\n",
    "\n",
    "disaster_length = train_full[train_full['target'] == 1]['text'].apply(lambda x: len(x))\n",
    "non_disaster_length = train_full[train_full['target'] == 0]['text'].apply(lambda x: len(x))\n",
    "\n",
    "axs[0].hist(x = disaster_length, color='red', bins=20, range = (0,160))\n",
    "axs[0].set_title('Length distribution of disaster tweets')\n",
    "axs[1].hist(x = non_disaster_length, bins=20, range = (0,160))\n",
    "axs[1].set_title('Length distribution of non-disaster tweets')\n",
    "axs[1].set_xlabel('Tweet length')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(f'Average length of disaster tweets is {round(disaster_length.mean())} ' \\\n",
    " f'with standard deviation {round(disaster_length.std())}')\n",
    "\n",
    "print(f'Average length of non-disaster tweets is {round(non_disaster_length.mean())} ' \\\n",
    " f'with standard deviation {round(non_disaster_length.std())}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the do seem to be slightly longer, but not by a huge amount."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of words\n",
    "Similarily, we want to understand if there are any key differences in the number of words in the tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length of disaster tweets is 19 with standard deviation 6\n",
      "Average length of non-disaster tweets is 19 with standard deviation 7\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "fig, axs = plt.subplots(2)\n",
    "fig. tight_layout(pad=3.0)\n",
    "\n",
    "disaster_wc = train_full[train_full['target'] == 1]['text'].apply(lambda x: len(word_tokenize(x)))\n",
    "non_disaster_wc = train_full[train_full['target'] == 0]['text'].apply(lambda x: len(word_tokenize(x)))\n",
    "\n",
    "axs[0].hist(x = disaster_wc, color='red', bins=20, range = (0,40))\n",
    "axs[0].set_title('Word count distribution of disaster tweets')\n",
    "axs[1].hist(x = non_disaster_wc, bins=20, range = (0,40))\n",
    "axs[1].set_title('Word count distribution of non-disaster tweets')\n",
    "axs[1].set_xlabel('Tweet word count')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(f'Average length of disaster tweets is {round(disaster_wc.mean())} ' \\\n",
    " f'with standard deviation {round(disaster_wc.std())}')\n",
    "\n",
    "print(f'Average length of non-disaster tweets is {round(non_disaster_wc.mean())} ' \\\n",
    " f'with standard deviation {round(non_disaster_wc.std())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, no real difference here between disaster and non-disaster tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Location\n",
    "\n",
    "Location data is provided for 65% of the samples. The location data seems to range from being as broad as a country, or as specific as the name of a restaurant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Percentage of tweets without a location: 66.73%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Birmingham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Est. September 2012 - Bristol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AFRICA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Philadelphia, PA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3337</th>\n",
       "      <td>TN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3338</th>\n",
       "      <td>#NewcastleuponTyne #UK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3339</th>\n",
       "      <td>Vancouver, Canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3340</th>\n",
       "      <td>London</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3341</th>\n",
       "      <td>Lincoln</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3342 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  0\n",
       "0                               NaN\n",
       "1                        Birmingham\n",
       "2     Est. September 2012 - Bristol\n",
       "3                            AFRICA\n",
       "4                  Philadelphia, PA\n",
       "...                             ...\n",
       "3337                             TN\n",
       "3338         #NewcastleuponTyne #UK\n",
       "3339              Vancouver, Canada\n",
       "3340                        London \n",
       "3341                        Lincoln\n",
       "\n",
       "[3342 rows x 1 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f' Percentage of tweets without a location: ' + \n",
    "      f'{round((1 - train_full.location.isna().sum() / train_full.shape[0]) * 100 , 2)}%')\n",
    "\n",
    "pd.DataFrame(train_full.location.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use CountVectorizer and BernoulliNB to create a baseline model, and get benchmark score against the validation set.\n",
    "\n",
    "Note that the target labels are binary outcomes, so they comrpise a Bernoulli distribution where  𝑝≈0.43 . Therefore accuracy is the chosen metric of performance, since there isn't a huge class imbalance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and validation data\n",
    "We will reserve 20% of the training data set as a validation data set to test iterations of the model against."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data set contains 6090 samples\n",
      "Validation data set contains 1523 samples\n"
     ]
    }
   ],
   "source": [
    "# Shuffle the index in case there are patterns in the order of the training data:\n",
    "np.random.seed(42)\n",
    "shuffled_ix = list(train_full.index)\n",
    "np.random.shuffle(shuffled_ix)\n",
    "cutoff = int(len(shuffled_ix)*0.8)\n",
    "train = train_full.loc[shuffled_ix[:cutoff]].copy()\n",
    "validation = train_full.loc[shuffled_ix[cutoff:]].copy()\n",
    "\n",
    "print(f'Train data set contains {train.shape[0]} samples')\n",
    "print(f'Validation data set contains {validation.shape[0]} samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train[\"text\"]\n",
    "train_labels = train[\"target\"]\n",
    "validation_data = validation[\"text\"]\n",
    "validation_labels = validation[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import plot_confusion_matrix, f1_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessing the baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli distribution, p = 0.430\n",
      "Validation f1 score = 0.784\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "vec = CountVectorizer()\n",
    "vec.fit(train_data)\n",
    "train_vec = vec.transform(train_data)\n",
    "\n",
    "print(f\"Bernoulli distribution, p = {train_labels.mean():.3f}\")\n",
    "\n",
    "model = BernoulliNB()\n",
    "crossval = cross_validate(model, train_vec, train_labels, cv=5, return_estimator=True)\n",
    "mean_score = crossval[\"test_score\"].mean()\n",
    "best_estimator = crossval[\"estimator\"][crossval[\"test_score\"].argmax()]\n",
    "validation_vec = vec.transform(validation_data)\n",
    "pred = best_estimator.predict(validation_vec)\n",
    "validation_score = f1_score(validation_labels, pred, average='weighted')\n",
    "print(f\"Validation f1 score = {validation_score:.3f}\")\n",
    "\n",
    "cm = plot_confusion_matrix(best_estimator, validation_vec, validation_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The baseline gives us an f1 score on the validation data set of 0.784, we can see from the confusion matrix that there are a lot of false negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def false_negatives(labels, preds, original_data, processed_data):\n",
    "    df = pd.DataFrame()\n",
    "    df['original_data'] = original_data\n",
    "    df['processed_data'] = processed_data\n",
    "    df['label'] = labels\n",
    "    df['prediction'] = preds\n",
    "    return df[(df[\"label\"] == 1) & (df[\"prediction\"] == 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def false_positives(labels, preds, original_data, processed_data):\n",
    "    df = pd.DataFrame()\n",
    "    df['original_data'] = original_data\n",
    "    df['processed_data'] = processed_data\n",
    "    df['label'] = labels\n",
    "    df['prediction'] = pred\n",
    "    return df[(df[\"label\"] == 0) & (df[\"prediction\"] == 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>processed_data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1279</th>\n",
       "      <td>Burned dog finds new home with young burn victim http://t.co/Pqrjvgvgxg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6883</th>\n",
       "      <td>@PyramidHead76 one good thing came out of watching the film.  Was too traumatised to watch show so started Halt &amp;amp; Catch Fire on Amazon. :D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7138</th>\n",
       "      <td>@MrMikeEaton @Muazimus_Prime hill hill mountain volcano of hell mountain hill hil.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3156</th>\n",
       "      <td>Gonna call up tomorrow with the aul 'emergency dental appointment' excuse just like the whole tooth falling out incident of last year</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5207</th>\n",
       "      <td>@TheEvilOlives It's the closest structure to the hypo centre that wasn't completely obliterated.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3310</th>\n",
       "      <td>Sooo police dispatch said there was a person threatening to shoot up the Walmart on Rutherford &amp;amp; they had to evacuate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3462</th>\n",
       "      <td>oh yeah my ipod almost exploded last night i was using it while charging and shit was sparking akxbskdn almost died</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2161</th>\n",
       "      <td>Bigamist and his 'first' wife are charged in the deaths of his 'second' pregnant wife her child 8 her mothe... http://t.co/rTEuGB5Tnv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>And so it begins.. day one of the snow apocalypse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4524</th>\n",
       "      <td>@Hurricane_Dolce no prob</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                      processed_data\n",
       "1279                                                                         Burned dog finds new home with young burn victim http://t.co/Pqrjvgvgxg\n",
       "6883  @PyramidHead76 one good thing came out of watching the film.  Was too traumatised to watch show so started Halt &amp; Catch Fire on Amazon. :D\n",
       "7138                                                              @MrMikeEaton @Muazimus_Prime hill hill mountain volcano of hell mountain hill hil.\n",
       "3156           Gonna call up tomorrow with the aul 'emergency dental appointment' excuse just like the whole tooth falling out incident of last year\n",
       "5207                                                @TheEvilOlives It's the closest structure to the hypo centre that wasn't completely obliterated.\n",
       "3310                       Sooo police dispatch said there was a person threatening to shoot up the Walmart on Rutherford &amp; they had to evacuate\n",
       "3462                             oh yeah my ipod almost exploded last night i was using it while charging and shit was sparking akxbskdn almost died\n",
       "2161           Bigamist and his 'first' wife are charged in the deaths of his 'second' pregnant wife her child 8 her mothe... http://t.co/rTEuGB5Tnv\n",
       "285                                                                                                And so it begins.. day one of the snow apocalypse\n",
       "4524                                                                                                                        @Hurricane_Dolce no prob"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "false_negatives(validation_labels, pred, validation_data, \n",
    "                validation_data)['processed_data'].to_frame()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the false positives there are a lot of hyperlinks, punctuation and @ symbols that can be cleaned up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1 - Naive Bayes\n",
    "\n",
    "We will clean the data up and use stemming an lematization to see if we can improve upon the baseline Naive Bayes model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/icexelloss/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/icexelloss/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "stemmer = PorterStemmer()\n",
    "lemmer = WordNetLemmatizer()\n",
    "\n",
    "def stem_sentence(text):\n",
    "    token_words=word_tokenize(text)\n",
    "    stem_sentence=[]\n",
    "    for word in token_words:\n",
    "        stem_sentence.append(stemmer.stem(word))\n",
    "        stem_sentence.append(\" \")\n",
    "    return \"\".join(stem_sentence)\n",
    "\n",
    "def lem_sentence(text):\n",
    "    token_words=word_tokenize(text)\n",
    "    stem_sentence=[]\n",
    "    for word in token_words:\n",
    "        stem_sentence.append(lemmer.lemmatize(word, pos='n'))\n",
    "        stem_sentence.append(\" \")\n",
    "    return \"\".join(stem_sentence)\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text)\n",
    "    text = re.sub(r'http?:\\/\\/.*[\\r\\n]*', '', text)\n",
    "    text = re.sub(r\"(?:\\@)\\w+\", ' ', text)\n",
    "    text = re.sub(r'[:\"#$%&\\*+,-/:;<=>@\\\\^_`{|}~?!]+',' ',text)\n",
    "    text = re.sub(r\"[']+\",'',text)\n",
    "    \n",
    "    text=text.replace(r'&amp;?',r'and')\n",
    "    text=text.replace(r'&lt;',r'<')\n",
    "    text=text.replace(r'&gt;',r'>')\n",
    "    \n",
    "    text=text.encode(\"ascii\",errors=\"ignore\").decode()\n",
    "\n",
    "    return text\n",
    "\n",
    "def clean_and_lem(text):\n",
    "    return lem_sentence(clean_text(text))\n",
    "\n",
    "def clean_and_stem(text):\n",
    "    return stem_sentence(clean_text(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean tweets text\n",
    "We clean the text to try to remove anything that adds noise, for example hyperlinks. \n",
    "In addition to this we've added Laplace smoothing to the model (alpha parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation f1 score = 0.797\n"
     ]
    }
   ],
   "source": [
    "vec = CountVectorizer(analyzer='word', preprocessor=clean_text)\n",
    "vec.fit(train_data)\n",
    "train_vec = vec.transform(train_data)\n",
    "\n",
    "model = BernoulliNB(alpha=0.85)\n",
    "crossval = cross_validate(model, train_vec, train_labels, cv=5, return_estimator=True)\n",
    "mean_score = crossval[\"test_score\"].mean()\n",
    "best_estimator = crossval[\"estimator\"][crossval[\"test_score\"].argmax()]\n",
    "validation_vec = vec.transform(validation_data)\n",
    "pred = best_estimator.predict(validation_vec)\n",
    "validation_score = f1_score(validation_labels, pred, average='weighted')\n",
    "print(f\"Validation f1 score = {validation_score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning the data has improved the f1 score on the validation data set to 0.797"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "We use the Porter stemmer to normalize words in the tweets. A stemming algorithm reduces a word to it's root, for example “retrieval”, “retrieved”, “retrieves” reduce to the stem “retrieve”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation f1 score = 0.796\n"
     ]
    }
   ],
   "source": [
    "vec = CountVectorizer(analyzer='word', preprocessor=clean_and_stem)\n",
    "vec.fit(train_data)\n",
    "train_vec = vec.transform(train_data)\n",
    "\n",
    "model = BernoulliNB(alpha=0.85)\n",
    "crossval = cross_validate(model, train_vec, train_labels, cv=5, return_estimator=True)\n",
    "mean_score = crossval[\"test_score\"].mean()\n",
    "best_estimator = crossval[\"estimator\"][crossval[\"test_score\"].argmax()]\n",
    "validation_vec = vec.transform(validation_data)\n",
    "pred = best_estimator.predict(validation_vec)\n",
    "validation_score = f1_score(validation_labels, pred, average='weighted')\n",
    "print(f\"Validation f1 score = {validation_score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying stemming to the data after cleaning it has reduced the accuracy of the model slightly, this may be due to overstemming where two words of different meaning are reduced to the same stem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "Lemmatization is similar to stemming but it converts a word to it's base form while also taking into account the context of the words. For example using lemmatization, 'caring' gets converted to 'care', whereas when using a stemmer, it will likely get converted to 'car'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy on 5-fold cross validation = 0.802\n",
      "Validation f1 score = 0.802\n"
     ]
    }
   ],
   "source": [
    "vec = CountVectorizer(analyzer='word', preprocessor=clean_and_lem)\n",
    "vec.fit(train_data)\n",
    "train_vec = vec.transform(train_data)\n",
    "\n",
    "model = BernoulliNB(alpha=0.75)\n",
    "crossval = cross_validate(model, train_vec, train_labels, cv=5, return_estimator=True)\n",
    "mean_score = crossval[\"test_score\"].mean()\n",
    "print(f\"Mean accuracy on 5-fold cross validation = {mean_score:.3f}\")\n",
    "best_estimator = crossval[\"estimator\"][crossval[\"test_score\"].argmax()]\n",
    "validation_vec = vec.transform(validation_data)\n",
    "pred = best_estimator.predict(validation_vec)\n",
    "validation_score = f1_score(validation_labels, pred, average='weighted')\n",
    "print(f\"Validation f1 score = {validation_score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatizing has been more successful and has increased the validation f1 score to 0.802"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term Frequency - Inverse Document Frequency \n",
    "TFIDF is based on the same bag of words technique as Count Vectorizer but it considers how important the word is. It does this by determining how rare the word is by diving the number of times is appears in the document by the number of times it appears in any document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy on 5-fold cross validation = 0.803\n",
      "Validation f1 score = 0.801\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vec = TfidfVectorizer(preprocessor=clean_and_lem, encoding='utf-8',\n",
    "                       ngram_range=(1,1),\n",
    "                       max_features=11000, \n",
    "                       norm='l2'\n",
    "                    )\n",
    "vec.fit(train_data)\n",
    "train_vec = vec.transform(train_data)\n",
    "\n",
    "model = BernoulliNB(alpha=0.75)\n",
    "crossval = cross_validate(model, train_vec, train_labels, cv=5, return_estimator=True)\n",
    "mean_score = crossval[\"test_score\"].mean()\n",
    "print(f\"Mean accuracy on 5-fold cross validation = {mean_score:.3f}\")\n",
    "best_estimator = crossval[\"estimator\"][crossval[\"test_score\"].argmax()]\n",
    "validation_vec = vec.transform(validation_data)\n",
    "pred = best_estimator.predict(validation_vec)\n",
    "validation_score = f1_score(validation_labels, pred, average='weighted')\n",
    "print(f\"Validation f1 score = {validation_score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_data</th>\n",
       "      <th>processed_data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1279</th>\n",
       "      <td>Burned dog finds new home with young burn victim http://t.co/Pqrjvgvgxg</td>\n",
       "      <td>burned dog find new home with young burn victim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6883</th>\n",
       "      <td>@PyramidHead76 one good thing came out of watching the film.  Was too traumatised to watch show so started Halt &amp;amp; Catch Fire on Amazon. :D</td>\n",
       "      <td>one good thing came out of watching the film wa too traumatised to watch show so started halt amp catch fire on amazon d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7138</th>\n",
       "      <td>@MrMikeEaton @Muazimus_Prime hill hill mountain volcano of hell mountain hill hil.</td>\n",
       "      <td>hill hill mountain volcano of hell mountain hill hil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3156</th>\n",
       "      <td>Gonna call up tomorrow with the aul 'emergency dental appointment' excuse just like the whole tooth falling out incident of last year</td>\n",
       "      <td>gon na call up tomorrow with the aul emergency dental appointment excuse just like the whole tooth falling out incident of last year</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5207</th>\n",
       "      <td>@TheEvilOlives It's the closest structure to the hypo centre that wasn't completely obliterated.</td>\n",
       "      <td>it the closest structure to the hypo centre that wasnt completely obliterated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3462</th>\n",
       "      <td>oh yeah my ipod almost exploded last night i was using it while charging and shit was sparking akxbskdn almost died</td>\n",
       "      <td>oh yeah my ipod almost exploded last night i wa using it while charging and shit wa sparking akxbskdn almost died</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2161</th>\n",
       "      <td>Bigamist and his 'first' wife are charged in the deaths of his 'second' pregnant wife her child 8 her mothe... http://t.co/rTEuGB5Tnv</td>\n",
       "      <td>bigamist and his first wife are charged in the death of his second pregnant wife her child 8 her mothe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>And so it begins.. day one of the snow apocalypse</td>\n",
       "      <td>and so it begin day one of the snow apocalypse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4524</th>\n",
       "      <td>@Hurricane_Dolce no prob</td>\n",
       "      <td>no prob</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6771</th>\n",
       "      <td>Maybe that's what happens when a tornado meets a volcano</td>\n",
       "      <td>maybe thats what happens when a tornado meet a volcano</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2717</th>\n",
       "      <td>Zayn Malik &amp;amp; Perrie Edwards End Engagement: SheÛªs Û÷DevastatedÛª http://t.co/GedOxSPpL9 http://t.co/ACZRUOrYtD</td>\n",
       "      <td>zayn malik amp perrie edward end engagement shes devastated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3212</th>\n",
       "      <td>We're #hiring! Click to apply: RN II/EMERGENCY SERVICES/FT/7P-7A - http://t.co/NV3Uxv9IMX #Nursing #Houston TX http://t.co/ej30IhrEA9</td>\n",
       "      <td>were hiring click to apply rn ii emergency service ft 7p 7a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6707</th>\n",
       "      <td>@HaydnExists so glad i saved them all at once then didnÛªt want you stealing my thunder :P</td>\n",
       "      <td>so glad i saved them all at once then didnt want you stealing my thunder p</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4120</th>\n",
       "      <td>Summer heat drives bobcats to Calgary backyards ~ 35 http://t.co/TmzXopVs94 http://t.co/W192Wkog1M</td>\n",
       "      <td>summer heat drive bobcat to calgary backyard 35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5298</th>\n",
       "      <td>@SenateMajLdr let's try to do our best to prevent another outbreak of violence by talking to each other both the people and the politics</td>\n",
       "      <td>let try to do our best to prevent another outbreak of violence by talking to each other both the people and the politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>http://t.co/FueRk0gWui Twelve feared killed in Pakistani air ambulance helicopter crash http://t.co/Mv7GgGlmVc</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5031</th>\n",
       "      <td>@Lolly_Knickers It's a mudslide. \\nIt's like chewing on a rubber tyre.\\nAnd with those I'm DONE.\\n#vaginaorcake #GBBO</td>\n",
       "      <td>it a mudslide it like chewing on a rubber tyre and with those im done vaginaorcake gbbo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2872</th>\n",
       "      <td>The Drought Is Real ??????</td>\n",
       "      <td>the drought is real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3030</th>\n",
       "      <td>#3Novices : Renison mine sees seismic event http://t.co/2i4EOGGO5j A small earthquake at Tasmania's Renison tin project has created a temÛ_</td>\n",
       "      <td>3novices renison mine see seismic event</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645</th>\n",
       "      <td>@durrellb Prices here are insane. Our dollar has collapsed against the US and it's punishing us. Thanks for the info.</td>\n",
       "      <td>price here are insane our dollar ha collapsed against the u and it punishing u thanks for the info</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                       original_data  \\\n",
       "1279                                                                         Burned dog finds new home with young burn victim http://t.co/Pqrjvgvgxg   \n",
       "6883  @PyramidHead76 one good thing came out of watching the film.  Was too traumatised to watch show so started Halt &amp; Catch Fire on Amazon. :D   \n",
       "7138                                                              @MrMikeEaton @Muazimus_Prime hill hill mountain volcano of hell mountain hill hil.   \n",
       "3156           Gonna call up tomorrow with the aul 'emergency dental appointment' excuse just like the whole tooth falling out incident of last year   \n",
       "5207                                                @TheEvilOlives It's the closest structure to the hypo centre that wasn't completely obliterated.   \n",
       "3462                             oh yeah my ipod almost exploded last night i was using it while charging and shit was sparking akxbskdn almost died   \n",
       "2161           Bigamist and his 'first' wife are charged in the deaths of his 'second' pregnant wife her child 8 her mothe... http://t.co/rTEuGB5Tnv   \n",
       "285                                                                                                And so it begins.. day one of the snow apocalypse   \n",
       "4524                                                                                                                        @Hurricane_Dolce no prob   \n",
       "6771                                                                                        Maybe that's what happens when a tornado meets a volcano   \n",
       "2717                          Zayn Malik &amp; Perrie Edwards End Engagement: SheÛªs Û÷DevastatedÛª http://t.co/GedOxSPpL9 http://t.co/ACZRUOrYtD   \n",
       "3212           We're #hiring! Click to apply: RN II/EMERGENCY SERVICES/FT/7P-7A - http://t.co/NV3Uxv9IMX #Nursing #Houston TX http://t.co/ej30IhrEA9   \n",
       "6707                                                     @HaydnExists so glad i saved them all at once then didnÛªt want you stealing my thunder :P   \n",
       "4120                                              Summer heat drives bobcats to Calgary backyards ~ 35 http://t.co/TmzXopVs94 http://t.co/W192Wkog1M   \n",
       "5298        @SenateMajLdr let's try to do our best to prevent another outbreak of violence by talking to each other both the people and the politics   \n",
       "201                                   http://t.co/FueRk0gWui Twelve feared killed in Pakistani air ambulance helicopter crash http://t.co/Mv7GgGlmVc   \n",
       "5031                           @Lolly_Knickers It's a mudslide. \\nIt's like chewing on a rubber tyre.\\nAnd with those I'm DONE.\\n#vaginaorcake #GBBO   \n",
       "2872                                                                                                                      The Drought Is Real ??????   \n",
       "3030    #3Novices : Renison mine sees seismic event http://t.co/2i4EOGGO5j A small earthquake at Tasmania's Renison tin project has created a temÛ_   \n",
       "1645                           @durrellb Prices here are insane. Our dollar has collapsed against the US and it's punishing us. Thanks for the info.   \n",
       "\n",
       "                                                                                                                             processed_data  \n",
       "1279                                                                                       burned dog find new home with young burn victim   \n",
       "6883              one good thing came out of watching the film wa too traumatised to watch show so started halt amp catch fire on amazon d   \n",
       "7138                                                                                  hill hill mountain volcano of hell mountain hill hil   \n",
       "3156  gon na call up tomorrow with the aul emergency dental appointment excuse just like the whole tooth falling out incident of last year   \n",
       "5207                                                         it the closest structure to the hypo centre that wasnt completely obliterated   \n",
       "3462                     oh yeah my ipod almost exploded last night i wa using it while charging and shit wa sparking akxbskdn almost died   \n",
       "2161                                bigamist and his first wife are charged in the death of his second pregnant wife her child 8 her mothe   \n",
       "285                                                                                         and so it begin day one of the snow apocalypse   \n",
       "4524                                                                                                                               no prob   \n",
       "6771                                                                                maybe thats what happens when a tornado meet a volcano   \n",
       "2717                                                                           zayn malik amp perrie edward end engagement shes devastated   \n",
       "3212                                                                           were hiring click to apply rn ii emergency service ft 7p 7a   \n",
       "6707                                                            so glad i saved them all at once then didnt want you stealing my thunder p   \n",
       "4120                                                                                       summer heat drive bobcat to calgary backyard 35   \n",
       "5298              let try to do our best to prevent another outbreak of violence by talking to each other both the people and the politics   \n",
       "201                                                                                                                                          \n",
       "5031                                               it a mudslide it like chewing on a rubber tyre and with those im done vaginaorcake gbbo   \n",
       "2872                                                                                                                   the drought is real   \n",
       "3030                                                                                               3novices renison mine see seismic event   \n",
       "1645                                    price here are insane our dollar ha collapsed against the u and it punishing u thanks for the info   "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "false_negatives(validation_labels, pred, validation_data, \n",
    "                validation_data.apply(lambda x: clean_and_lem(x)))[['original_data', 'processed_data']][:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's interesting to see a lot of the tweets here that were originally labelled as disasters don't seem to be about disasters, so when examining these it seems like our model may be correctly classifying them as non-disaster tweets. We'll get predictions for the test data set provided by Kaggle using this model and see how it performs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessing using the test data set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vec = vec.transform(test['text'])\n",
    "test_preds = best_estimator.predict(test_vec)\n",
    "nb_submission = pd.DataFrame({'id': test.id, 'target': test_preds})\n",
    "#np.savetxt(\"naive_bayes_test_predictions.csv\", test_preds, delimiter=\",\", header=\"target\", \n",
    "      #     fmt=\"%i\")\n",
    "nb_submission.to_csv('naive_bayes_test_predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This submission gave us a Kaggle score of 0.79313"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2 - Logistic Regression\n",
    "\n",
    "In this section, we will introduce our second model using Logistic Regression and analysis the result. This section will use the same preprocessing as the previous model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model and Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy on 5-fold cross validation = 0.807\n",
      "Validation f1 score = 0.782\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(train_vec, train_labels)\n",
    "pred = model.predict(validation_vec)\n",
    "\n",
    "crossval = cross_validate(model, train_vec, train_labels, cv=5, return_estimator=True)\n",
    "mean_score = crossval[\"test_score\"].mean()\n",
    "print(f\"Mean accuracy on 5-fold cross validation = {mean_score:.3f}\")\n",
    "best_estimator = crossval[\"estimator\"][crossval[\"test_score\"].argmax()]\n",
    "validation_vec = vec.transform(validation_data)\n",
    "pred = best_estimator.predict(validation_vec)\n",
    "validation_score = f1_score(validation_labels, pred, average='weighted')\n",
    "print(f\"Validation f1 score = {validation_score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The score is a little worse than Model 1. Next we will do some error analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Top Coefficients\n",
    "\n",
    "First we look at highest coefficients to see if that makes sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table of top 50 features and their coefficients\n",
      "                positive\n",
      "(fire,)         3.810866\n",
      "(in,)           3.343749\n",
      "(hiroshima,)    3.085050\n",
      "(train,)        2.900931\n",
      "(california,)   2.779010\n",
      "(storm,)        2.644719\n",
      "(wildfire,)     2.558687\n",
      "(bombing,)      2.469046\n",
      "(flood,)        2.427287\n",
      "(suicide,)      2.357004\n",
      "(disaster,)     2.262189\n",
      "(near,)         2.096496\n",
      "(police,)       2.037451\n",
      "(building,)     1.990621\n",
      "(murder,)       1.962556\n",
      "(at,)           1.849177\n",
      "(over,)         1.842410\n",
      "(typhoon,)      1.841772\n",
      "(mass,)         1.828538\n",
      "(after,)        1.816829\n",
      "(war,)          1.811453\n",
      "(japan,)        1.800284\n",
      "(derailment,)   1.797760\n",
      "(drought,)      1.795589\n",
      "(warning,)      1.792510\n",
      "(crash,)        1.783022\n",
      "(terrorist,)    1.726204\n",
      "(earthquake,)   1.721908\n",
      "(collapse,)     1.709789\n",
      "(debris,)       1.703589\n",
      "(car,)          1.698958\n",
      "(killed,)       1.696351\n",
      "(massacre,)     1.684230\n",
      "(tornado,)      1.667455\n",
      "(mh370,)        1.660156\n",
      "(nuclear,)      1.648894\n",
      "(during,)       1.638216\n",
      "(home,)         1.633712\n",
      "(migrant,)      1.629718\n",
      "(severe,)       1.592537\n",
      "(casualty,)     1.548728\n",
      "(of,)           1.538940\n",
      "(were,)         1.531217\n",
      "(atomic,)       1.502200\n",
      "(evacuation,)   1.489263\n",
      "(spill,)        1.486331\n",
      "(bridge,)       1.480041\n",
      "(forest,)       1.465655\n",
      "(legionnaire,)  1.464819\n",
      "(area,)         1.461219\n"
     ]
    }
   ],
   "source": [
    "feature_index = np.argsort(-model.coef_)[:, :100].flatten()\n",
    "\n",
    "vocabulary_df = pd.DataFrame({\n",
    "    'text': vec.vocabulary_.keys(),\n",
    "    'feature': vec.vocabulary_.values()\n",
    "}).set_index('feature')\n",
    "\n",
    "df = pd.DataFrame(model.coef_[:, feature_index]).T\n",
    "df.index = vocabulary_df.loc[feature_index]\n",
    "df.columns = ['positive']\n",
    "\n",
    "print(\"Table of top 50 features and their coefficients\")\n",
    "print(df.iloc[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At a glance the coefficients are a mixture of \n",
    "* indicator of a disaster (wildfire, hailstorm, earthquake, ...) \n",
    "* words that can be related to a disaster (catastrophic, police, severe, ...)\n",
    "* Neutral word (airport, coach, gas, near, ...)\n",
    "\n",
    "which seem to make sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### False Negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_data</th>\n",
       "      <th>processed_data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1279</th>\n",
       "      <td>Burned dog finds new home with young burn victim http://t.co/Pqrjvgvgxg</td>\n",
       "      <td>burned dog find new home with young burn victim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6883</th>\n",
       "      <td>@PyramidHead76 one good thing came out of watching the film.  Was too traumatised to watch show so started Halt &amp;amp; Catch Fire on Amazon. :D</td>\n",
       "      <td>one good thing came out of watching the film wa too traumatised to watch show so started halt amp catch fire on amazon d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7138</th>\n",
       "      <td>@MrMikeEaton @Muazimus_Prime hill hill mountain volcano of hell mountain hill hil.</td>\n",
       "      <td>hill hill mountain volcano of hell mountain hill hil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3156</th>\n",
       "      <td>Gonna call up tomorrow with the aul 'emergency dental appointment' excuse just like the whole tooth falling out incident of last year</td>\n",
       "      <td>gon na call up tomorrow with the aul emergency dental appointment excuse just like the whole tooth falling out incident of last year</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5207</th>\n",
       "      <td>@TheEvilOlives It's the closest structure to the hypo centre that wasn't completely obliterated.</td>\n",
       "      <td>it the closest structure to the hypo centre that wasnt completely obliterated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3462</th>\n",
       "      <td>oh yeah my ipod almost exploded last night i was using it while charging and shit was sparking akxbskdn almost died</td>\n",
       "      <td>oh yeah my ipod almost exploded last night i wa using it while charging and shit wa sparking akxbskdn almost died</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4895</th>\n",
       "      <td>Headed to the massacre \\nBodies arriving everyday \\nWhat were those shells you heard \\nPicking the bones up along the way</td>\n",
       "      <td>headed to the massacre body arriving everyday what were those shell you heard picking the bone up along the way</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2161</th>\n",
       "      <td>Bigamist and his 'first' wife are charged in the deaths of his 'second' pregnant wife her child 8 her mothe... http://t.co/rTEuGB5Tnv</td>\n",
       "      <td>bigamist and his first wife are charged in the death of his second pregnant wife her child 8 her mothe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>And so it begins.. day one of the snow apocalypse</td>\n",
       "      <td>and so it begin day one of the snow apocalypse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4524</th>\n",
       "      <td>@Hurricane_Dolce no prob</td>\n",
       "      <td>no prob</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6771</th>\n",
       "      <td>Maybe that's what happens when a tornado meets a volcano</td>\n",
       "      <td>maybe thats what happens when a tornado meet a volcano</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2717</th>\n",
       "      <td>Zayn Malik &amp;amp; Perrie Edwards End Engagement: SheÛªs Û÷DevastatedÛª http://t.co/GedOxSPpL9 http://t.co/ACZRUOrYtD</td>\n",
       "      <td>zayn malik amp perrie edward end engagement shes devastated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3212</th>\n",
       "      <td>We're #hiring! Click to apply: RN II/EMERGENCY SERVICES/FT/7P-7A - http://t.co/NV3Uxv9IMX #Nursing #Houston TX http://t.co/ej30IhrEA9</td>\n",
       "      <td>were hiring click to apply rn ii emergency service ft 7p 7a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6707</th>\n",
       "      <td>@HaydnExists so glad i saved them all at once then didnÛªt want you stealing my thunder :P</td>\n",
       "      <td>so glad i saved them all at once then didnt want you stealing my thunder p</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4120</th>\n",
       "      <td>Summer heat drives bobcats to Calgary backyards ~ 35 http://t.co/TmzXopVs94 http://t.co/W192Wkog1M</td>\n",
       "      <td>summer heat drive bobcat to calgary backyard 35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5298</th>\n",
       "      <td>@SenateMajLdr let's try to do our best to prevent another outbreak of violence by talking to each other both the people and the politics</td>\n",
       "      <td>let try to do our best to prevent another outbreak of violence by talking to each other both the people and the politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>http://t.co/FueRk0gWui Twelve feared killed in Pakistani air ambulance helicopter crash http://t.co/Mv7GgGlmVc</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3502</th>\n",
       "      <td>You Are Invited to The Expo Explosion Summer Event 2015! \\nWHEN: August 14th Friday 2015\\nWHERE: Ben E Keith... http://t.co/yh4R7Ug21a</td>\n",
       "      <td>you are invited to the expo explosion summer event 2015 when august 14th friday 2015 where ben e keith</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5031</th>\n",
       "      <td>@Lolly_Knickers It's a mudslide. \\nIt's like chewing on a rubber tyre.\\nAnd with those I'm DONE.\\n#vaginaorcake #GBBO</td>\n",
       "      <td>it a mudslide it like chewing on a rubber tyre and with those im done vaginaorcake gbbo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2872</th>\n",
       "      <td>The Drought Is Real ??????</td>\n",
       "      <td>the drought is real</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                       original_data  \\\n",
       "1279                                                                         Burned dog finds new home with young burn victim http://t.co/Pqrjvgvgxg   \n",
       "6883  @PyramidHead76 one good thing came out of watching the film.  Was too traumatised to watch show so started Halt &amp; Catch Fire on Amazon. :D   \n",
       "7138                                                              @MrMikeEaton @Muazimus_Prime hill hill mountain volcano of hell mountain hill hil.   \n",
       "3156           Gonna call up tomorrow with the aul 'emergency dental appointment' excuse just like the whole tooth falling out incident of last year   \n",
       "5207                                                @TheEvilOlives It's the closest structure to the hypo centre that wasn't completely obliterated.   \n",
       "3462                             oh yeah my ipod almost exploded last night i was using it while charging and shit was sparking akxbskdn almost died   \n",
       "4895                       Headed to the massacre \\nBodies arriving everyday \\nWhat were those shells you heard \\nPicking the bones up along the way   \n",
       "2161           Bigamist and his 'first' wife are charged in the deaths of his 'second' pregnant wife her child 8 her mothe... http://t.co/rTEuGB5Tnv   \n",
       "285                                                                                                And so it begins.. day one of the snow apocalypse   \n",
       "4524                                                                                                                        @Hurricane_Dolce no prob   \n",
       "6771                                                                                        Maybe that's what happens when a tornado meets a volcano   \n",
       "2717                          Zayn Malik &amp; Perrie Edwards End Engagement: SheÛªs Û÷DevastatedÛª http://t.co/GedOxSPpL9 http://t.co/ACZRUOrYtD   \n",
       "3212           We're #hiring! Click to apply: RN II/EMERGENCY SERVICES/FT/7P-7A - http://t.co/NV3Uxv9IMX #Nursing #Houston TX http://t.co/ej30IhrEA9   \n",
       "6707                                                     @HaydnExists so glad i saved them all at once then didnÛªt want you stealing my thunder :P   \n",
       "4120                                              Summer heat drives bobcats to Calgary backyards ~ 35 http://t.co/TmzXopVs94 http://t.co/W192Wkog1M   \n",
       "5298        @SenateMajLdr let's try to do our best to prevent another outbreak of violence by talking to each other both the people and the politics   \n",
       "201                                   http://t.co/FueRk0gWui Twelve feared killed in Pakistani air ambulance helicopter crash http://t.co/Mv7GgGlmVc   \n",
       "3502          You Are Invited to The Expo Explosion Summer Event 2015! \\nWHEN: August 14th Friday 2015\\nWHERE: Ben E Keith... http://t.co/yh4R7Ug21a   \n",
       "5031                           @Lolly_Knickers It's a mudslide. \\nIt's like chewing on a rubber tyre.\\nAnd with those I'm DONE.\\n#vaginaorcake #GBBO   \n",
       "2872                                                                                                                      The Drought Is Real ??????   \n",
       "\n",
       "                                                                                                                             processed_data  \n",
       "1279                                                                                       burned dog find new home with young burn victim   \n",
       "6883              one good thing came out of watching the film wa too traumatised to watch show so started halt amp catch fire on amazon d   \n",
       "7138                                                                                  hill hill mountain volcano of hell mountain hill hil   \n",
       "3156  gon na call up tomorrow with the aul emergency dental appointment excuse just like the whole tooth falling out incident of last year   \n",
       "5207                                                         it the closest structure to the hypo centre that wasnt completely obliterated   \n",
       "3462                     oh yeah my ipod almost exploded last night i wa using it while charging and shit wa sparking akxbskdn almost died   \n",
       "4895                       headed to the massacre body arriving everyday what were those shell you heard picking the bone up along the way   \n",
       "2161                                bigamist and his first wife are charged in the death of his second pregnant wife her child 8 her mothe   \n",
       "285                                                                                         and so it begin day one of the snow apocalypse   \n",
       "4524                                                                                                                               no prob   \n",
       "6771                                                                                maybe thats what happens when a tornado meet a volcano   \n",
       "2717                                                                           zayn malik amp perrie edward end engagement shes devastated   \n",
       "3212                                                                           were hiring click to apply rn ii emergency service ft 7p 7a   \n",
       "6707                                                            so glad i saved them all at once then didnt want you stealing my thunder p   \n",
       "4120                                                                                       summer heat drive bobcat to calgary backyard 35   \n",
       "5298              let try to do our best to prevent another outbreak of violence by talking to each other both the people and the politics   \n",
       "201                                                                                                                                          \n",
       "3502                                you are invited to the expo explosion summer event 2015 when august 14th friday 2015 where ben e keith   \n",
       "5031                                               it a mudslide it like chewing on a rubber tyre and with those im done vaginaorcake gbbo   \n",
       "2872                                                                                                                   the drought is real   "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "false_negatives(validation_labels, pred, validation_data, \n",
    "                validation_data.apply(lambda x: clean_and_lem(x)))[['original_data', 'processed_data']][:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Looking at false negatives, it appears that some of the test labels can be wrong. For example, these tweets are clearly not disasters but is labeled as so:\n",
    "* but if you build an army of 100 dogs and their leader is a lion all dogs will fight like a lion\n",
    "* watch these super strong magnets destroy everyday objects\n",
    "* check out my lava lamp dude   \n",
    "* ...\n",
    "\n",
    "These wrong labels can explain why recall for positive is quite low."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### False positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_data</th>\n",
       "      <th>processed_data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3190</th>\n",
       "      <td>Practice your families fire escape plan so everyone knows what to do in case of an emergency.</td>\n",
       "      <td>practice your family fire escape plan so everyone know what to do in case of an emergency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3594</th>\n",
       "      <td>The Five Fatal Flaws in the Iran Deal https://t.co/ztfEAd8GId via @YouTube</td>\n",
       "      <td>the five fatal flaw in the iran deal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1308</th>\n",
       "      <td>RT @HuffPostComedy: We should build a wall that keeps Burning Man attendees from coming home http://t.co/xwVW1sft4I http://t.co/j7HUKhWmal</td>\n",
       "      <td>rt we should build a wall that keep burning man attendee from coming home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6097</th>\n",
       "      <td>?that horrible sinking feeling when youÛªve been at home on your phone for a while and you realise its been on 3G this whole time</td>\n",
       "      <td>that horrible sinking feeling when youve been at home on your phone for a while and you realise it been on 3g this whole time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5619</th>\n",
       "      <td>Newlyweds feed Syrian refugees at their wedding - CBC News - Latest Canada World Entertainment and Business News http://t.co/QU8S89pVVt</td>\n",
       "      <td>newlywed feed syrian refugee at their wedding cbc news latest canada world entertainment and business news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7159</th>\n",
       "      <td>Camping in a war zone with roving raccoons toughens city slicker http://t.co/oJuS08yZrq</td>\n",
       "      <td>camping in a war zone with roving raccoon toughens city slicker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6103</th>\n",
       "      <td>that horrible sinking feeling when youÛªve been at home on your phone for a while and you realise its been on 3G this whole time</td>\n",
       "      <td>that horrible sinking feeling when youve been at home on your phone for a while and you realise it been on 3g this whole time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4873</th>\n",
       "      <td>@BenignoVito @LibertyBell1000 HILLARYMASS MURDERER.</td>\n",
       "      <td>hillarymass murderer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5083</th>\n",
       "      <td>Nuclear deal disaster.\\n\\n#IranDeal #NoNuclearIran #BadIranDeal @JebBush @BarackObama http://t.co/z7phPjtqud</td>\n",
       "      <td>nuclear deal disaster irandeal nonucleariran badirandeal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3057</th>\n",
       "      <td>#Sismo M 1.3 - 1km NNE of The Geysers California: Time2015-08-05 23:40:21 UTC2015-08-05 16:40:21 -07:00 a... http://t.co/x6el3ySYcn #CS</td>\n",
       "      <td>sismo m 1 3 1km nne of the geyser california time2015 08 05 23 40 21 utc2015 08 05 16 40 21 07 00 a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6251</th>\n",
       "      <td>New #photo Oak in a snowstorm http://t.co/JhSCGDA2G8 on the #SouthDowns #Hampshire #Winter #photography #art #tree #treescape #treeporn</td>\n",
       "      <td>new photo oak in a snowstorm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6245</th>\n",
       "      <td>'Cooler than Freddie Jackson sippin' a milkshake in a snowstorm'</td>\n",
       "      <td>cooler than freddie jackson sippin a milkshake in a snowstorm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7100</th>\n",
       "      <td>Violent Forces Radio: Now Playing Agony - Storm of the apocalypse\\nTuneIn Player @ http://t.co/XsSgEdSbH4</td>\n",
       "      <td>violent force radio now playing agony storm of the apocalypse tunein player</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3797</th>\n",
       "      <td>Trident 90225 Chevy fire Truck w/ Pumper  IMS Fire Dept. Red HO 1:87  plastic http://t.co/FAQNFUpeGn http://t.co/mQfOfKXtyh</td>\n",
       "      <td>trident 90225 chevy fire truck w pumper ims fire dept red ho 1 87 plastic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3056</th>\n",
       "      <td>#USGS M 1.4 - 4km E of Interlaken California: Time2015-08-06 00:52:25 UTC2015-08-05 17:52:25 -07:00 at ep... http://t.co/zqrcptLrUM #SM</td>\n",
       "      <td>usgs m 1 4 4km e of interlaken california time2015 08 06 00 52 25 utc2015 08 05 17 52 25 07 00 at ep</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3876</th>\n",
       "      <td>The advantages apropos of in flames favorable regard mississauga ontario: pWHvGwax</td>\n",
       "      <td>the advantage apropos of in flame favorable regard mississauga ontario pwhvgwax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>.: .: .: .: .: .: .: .: .: .: .: .: .: .: .: .: .: .: .: .: .: RT DrAyesha4: #IndiaKoMunTorJawabDo\\n\\nIndian Army kiÛ_ http://t.co/WJLJq3yA4g</td>\n",
       "      <td>rt drayesha4 indiakomuntorjawabdo indian army ki</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5788</th>\n",
       "      <td>@CloydRivers there were plenty of black people rioting when tOSU won the championship as well.</td>\n",
       "      <td>there were plenty of black people rioting when tosu won the championship a well</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3687</th>\n",
       "      <td>Kosciusko police investigating pedestrian fatality hit by a train Thursday http://t.co/m5djLLxoZP</td>\n",
       "      <td>kosciusko police investigating pedestrian fatality hit by a train thursday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5142</th>\n",
       "      <td>A little filming inside a Nuclear Reactor at #Chernobyl @SonyProUSA @LumixUSA @DJIGlobal @ProfBrianCox @RT_America https://t.co/2GLjhvEAD9</td>\n",
       "      <td>a little filming inside a nuclear reactor at chernobyl</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                       original_data  \\\n",
       "3190                                                   Practice your families fire escape plan so everyone knows what to do in case of an emergency.   \n",
       "3594                                                                      The Five Fatal Flaws in the Iran Deal https://t.co/ztfEAd8GId via @YouTube   \n",
       "1308      RT @HuffPostComedy: We should build a wall that keeps Burning Man attendees from coming home http://t.co/xwVW1sft4I http://t.co/j7HUKhWmal   \n",
       "6097              ?that horrible sinking feeling when youÛªve been at home on your phone for a while and you realise its been on 3G this whole time   \n",
       "5619         Newlyweds feed Syrian refugees at their wedding - CBC News - Latest Canada World Entertainment and Business News http://t.co/QU8S89pVVt   \n",
       "7159                                                         Camping in a war zone with roving raccoons toughens city slicker http://t.co/oJuS08yZrq   \n",
       "6103               that horrible sinking feeling when youÛªve been at home on your phone for a while and you realise its been on 3G this whole time   \n",
       "4873                                                                                             @BenignoVito @LibertyBell1000 HILLARYMASS MURDERER.   \n",
       "5083                                    Nuclear deal disaster.\\n\\n#IranDeal #NoNuclearIran #BadIranDeal @JebBush @BarackObama http://t.co/z7phPjtqud   \n",
       "3057         #Sismo M 1.3 - 1km NNE of The Geysers California: Time2015-08-05 23:40:21 UTC2015-08-05 16:40:21 -07:00 a... http://t.co/x6el3ySYcn #CS   \n",
       "6251         New #photo Oak in a snowstorm http://t.co/JhSCGDA2G8 on the #SouthDowns #Hampshire #Winter #photography #art #tree #treescape #treeporn   \n",
       "6245                                                                                'Cooler than Freddie Jackson sippin' a milkshake in a snowstorm'   \n",
       "7100                                       Violent Forces Radio: Now Playing Agony - Storm of the apocalypse\\nTuneIn Player @ http://t.co/XsSgEdSbH4   \n",
       "3797                     Trident 90225 Chevy fire Truck w/ Pumper  IMS Fire Dept. Red HO 1:87  plastic http://t.co/FAQNFUpeGn http://t.co/mQfOfKXtyh   \n",
       "3056         #USGS M 1.4 - 4km E of Interlaken California: Time2015-08-06 00:52:25 UTC2015-08-05 17:52:25 -07:00 at ep... http://t.co/zqrcptLrUM #SM   \n",
       "3876                                                              The advantages apropos of in flames favorable regard mississauga ontario: pWHvGwax   \n",
       "362   .: .: .: .: .: .: .: .: .: .: .: .: .: .: .: .: .: .: .: .: .: RT DrAyesha4: #IndiaKoMunTorJawabDo\\n\\nIndian Army kiÛ_ http://t.co/WJLJq3yA4g   \n",
       "5788                                                  @CloydRivers there were plenty of black people rioting when tOSU won the championship as well.   \n",
       "3687                                               Kosciusko police investigating pedestrian fatality hit by a train Thursday http://t.co/m5djLLxoZP   \n",
       "5142      A little filming inside a Nuclear Reactor at #Chernobyl @SonyProUSA @LumixUSA @DJIGlobal @ProfBrianCox @RT_America https://t.co/2GLjhvEAD9   \n",
       "\n",
       "                                                                                                                      processed_data  \n",
       "3190                                      practice your family fire escape plan so everyone know what to do in case of an emergency   \n",
       "3594                                                                                           the five fatal flaw in the iran deal   \n",
       "1308                                                      rt we should build a wall that keep burning man attendee from coming home   \n",
       "6097  that horrible sinking feeling when youve been at home on your phone for a while and you realise it been on 3g this whole time   \n",
       "5619                     newlywed feed syrian refugee at their wedding cbc news latest canada world entertainment and business news   \n",
       "7159                                                                camping in a war zone with roving raccoon toughens city slicker   \n",
       "6103  that horrible sinking feeling when youve been at home on your phone for a while and you realise it been on 3g this whole time   \n",
       "4873                                                                                                           hillarymass murderer   \n",
       "5083                                                                       nuclear deal disaster irandeal nonucleariran badirandeal   \n",
       "3057                            sismo m 1 3 1km nne of the geyser california time2015 08 05 23 40 21 utc2015 08 05 16 40 21 07 00 a   \n",
       "6251                                                                                                   new photo oak in a snowstorm   \n",
       "6245                                                                  cooler than freddie jackson sippin a milkshake in a snowstorm   \n",
       "7100                                                    violent force radio now playing agony storm of the apocalypse tunein player   \n",
       "3797                                                      trident 90225 chevy fire truck w pumper ims fire dept red ho 1 87 plastic   \n",
       "3056                           usgs m 1 4 4km e of interlaken california time2015 08 06 00 52 25 utc2015 08 05 17 52 25 07 00 at ep   \n",
       "3876                                                the advantage apropos of in flame favorable regard mississauga ontario pwhvgwax   \n",
       "362                                                                                rt drayesha4 indiakomuntorjawabdo indian army ki   \n",
       "5788                                                there were plenty of black people rioting when tosu won the championship a well   \n",
       "3687                                                     kosciusko police investigating pedestrian fatality hit by a train thursday   \n",
       "5142                                                                         a little filming inside a nuclear reactor at chernobyl   "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "false_positives(validation_labels, pred, validation_data, \n",
    "                validation_data.apply(lambda x: clean_and_lem(x)))[['original_data', 'processed_data']][:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at false positives, some of the tweets are very hard for this model to detect, e.g:\n",
    "\n",
    "* battlefield 4 funny moments   dukes of hazard undercover soldier mav t  (video game)\n",
    "* shes a suicide bomb (metaphor)\n",
    "\n",
    "Some of the false positives are possibily due to the bags of word approach itself:\n",
    "\n",
    "* advice from noah  dont go running in a thunderstorm\n",
    "\n",
    "Some of the false postives might get better two a bigram approach:\n",
    "* the government is concerned about the population explosion and the population is concerned about the government explosion    joe moore (population explosion)\n",
    "* pizza drought is over i just couldnt anymore (pizza drought)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigram\n",
    "Finally, we try to use bigram and see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy on 5-fold cross validation = 0.705\n",
      "Validation f1 score = 0.709\n"
     ]
    }
   ],
   "source": [
    "# Warning, run this cell will change tra\n",
    "vec_2 = TfidfVectorizer(\n",
    "    preprocessor=clean_and_lem, encoding='utf-8',\n",
    "    ngram_range=(2, 2),\n",
    "    norm='l2'\n",
    ")\n",
    "train_vec_2 = vec_2.fit_transform(train_data)\n",
    "validation_vec_2 = vec_2.transform(validation_data)\n",
    "\n",
    "model_2 = LogisticRegression()\n",
    "model_2.fit(train_vec_2, train_labels)\n",
    "pred_2 = model_2.predict(validation_vec_2)\n",
    "\n",
    "crossval_2 = cross_validate(model_2, train_vec_2, train_labels, cv=5, return_estimator=True)\n",
    "mean_score_2 = crossval_2[\"test_score\"].mean()\n",
    "print(f\"Mean accuracy on 5-fold cross validation = {mean_score_2:.3f}\")\n",
    "best_estimator_2 = crossval_2[\"estimator\"][crossval_2[\"test_score\"].argmax()]\n",
    "pred_2 = best_estimator_2.predict(validation_vec_2)\n",
    "validation_score = f1_score(validation_labels, pred_2, average='weighted')\n",
    "print(f\"Validation f1 score = {validation_score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, using bigram results in a worse performance. Therefore we will stick to unigram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessing using the test data set¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vec = vec.transform(test['text'])\n",
    "test_preds = best_estimator.predict(test_vec)\n",
    "nb_submission = pd.DataFrame({'id': test.id, 'target': test_preds})\n",
    "nb_submission.to_csv('logistic_regression_test_predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us a final score of 0.78945."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3 - Neural Network\n",
    "\n",
    "The final approach we try is to use a different representation of text data as word embeddings, and to train a Neural Network model to classify the tweets based on numeric vector inputs created from these word embeddings.\n",
    "\n",
    "### GloVe Word Embeddings\n",
    "\n",
    "The word embeddings scheme chosen is a pretrained GloVe model with ~1.2m word vocabulary, trained on ~2 billion tweets. It is hoped that by using a word embeddings scheme trained on tweets, it will perform well on our dataset which is also scraped from Twitter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-299de924cdbb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdownloader\u001b[0m  \u001b[0;31m# Library which supplies the word embeddings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Load the pretrained word embeddings (this takes a while):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mglove_twitter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdownloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"glove-twitter-200\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdownloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "from gensim import downloader  # Library which supplies the word embeddings.\n",
    "\n",
    "# Load the pretrained word embeddings (this takes a while):\n",
    "glove_twitter = downloader.load(\"glove-twitter-200\")\n",
    "info = downloader.info()\n",
    "print(info[\"models\"][\"glove-twitter-200\"][\"description\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Processing\n",
    "\n",
    "The creators of the particular word embeddings scheme chosen also released a <a href=\"https://nlp.stanford.edu/projects/glove/preprocess-twitter.rb\">Ruby script</a> to apply the same text transformations to tweet data that they used when creating the embeddings. A user on Kaggle converted this script to <a href=\"https://www.kaggle.com/amackcrane/python-version-of-glove-twitter-preprocess-script\">Python</a>, which we adapted below with some additional changes to improve the number of input words which can be embedded. Some of these changes include:\n",
    "\n",
    "1. Removing most punctuation entirely. The original Kaggle script leaves a lot of punctation untouched, so that for example the quoted word `'something'` would be included as a separate feature to the unquoted word `something`. We make the assumption that such differences in punctuation of features are unlikely to be significant in identifying disaster tweets.\n",
    "2. Fixing a couple of bugs with the way whitespace was added around certain punctuation marks, such as `.,`\n",
    "3. Add spaces around inserted tags such as ` <number> `\n",
    "\n",
    "Making the above changes to the preprocessor we increase the number of input words which are found in the embeddings vocabulary from ~75% to nearly 90%.\n",
    "\n",
    "One interesting decision in the preprocessing script is to remove tagged twitter usernames (i.e. strings starting with `@`), since we might expect that this could provide some useful information in trying to predict the nature of the tweets. For example, a tweet tagging a local fire department might be much more likely to be a disaster related tweet. However since we are using a pretrained embeddings scheme we don't have the option to include usernames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import string\n",
    "import sys\n",
    "import regex as re\n",
    "\n",
    "FLAGS = re.MULTILINE | re.DOTALL\n",
    "\n",
    "\n",
    "punctuation = string.punctuation.replace(\"#\", \"\")\n",
    "punctuation = punctuation.replace(\"<\", \"\")\n",
    "punctuation = punctuation.replace(\">\", \"\")\n",
    "\n",
    "\n",
    "def _hashtag(text):\n",
    "    \"\"\"Parse hashtags in tweets.\n",
    "    \n",
    "    For hashtags with words in title case, splits into separate words, e.g.\n",
    "        #BigFire -> <hashtag> Big Fire \n",
    "        \n",
    "    For all caps hashtags converts to lowercase, e.g.\n",
    "        #OMG -> <hashtag> omg\n",
    "    \"\"\"\n",
    "    text = text.group()\n",
    "    hashtag_body = text[1:]\n",
    "    if hashtag_body.isupper():\n",
    "        result = f\" <hashtag> {hashtag_body.lower()} <allcaps> \"\n",
    "    else:\n",
    "        result = \" \".join([\" <hashtag> \"] + re.split(r\"(?=[A-Z])\", hashtag_body, flags=FLAGS))\n",
    "    return result\n",
    "\n",
    "\n",
    "def _allcaps(text):\n",
    "    \"\"\"Convert all-caps words to lowercase and add allcaps tag.\"\"\"\n",
    "    text = text.group()\n",
    "    return text.lower() + \" <allcaps> \"\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    def re_sub(pattern, repl):\n",
    "        return re.sub(pattern, repl, text, flags=FLAGS)\n",
    "\n",
    "    # Remove URLS:\n",
    "    text = re_sub(r\"https?:\\/\\/\\S+\\b|www\\.(\\w+\\.)+\\S*\", \" <url> \")\n",
    "    \n",
    "    # Remove usernames:\n",
    "    text = re_sub(r\"@\\w+\", \" <user> \")\n",
    "    \n",
    "    # Tag smiley faces :-) sad faces :-( etc.\n",
    "    eyes, nose = r\"[8:=;]\", r\"['`\\-]?\"\n",
    "    text = re_sub(r\"{}{}[)dD]+|[)dD]+{}{}\".format(eyes, nose, nose, eyes), \" <smile> \")\n",
    "    text = re_sub(r\"{}{}p+\".format(eyes, nose), \" <lolface> \")\n",
    "    text = re_sub(r\"{}{}\\(+|\\)+{}{}\".format(eyes, nose, nose, eyes), \" <sadface> \")\n",
    "    text = re_sub(r\"{}{}[\\/|l*]\".format(eyes, nose), \" <neutralface> \")\n",
    "    text = re_sub(r\"<3\",\" <heart> \")\n",
    "\n",
    "    # Add whitespace around /\n",
    "    text = re_sub(r\"/\",\" / \")\n",
    "    \n",
    "    # Tag numbers:\n",
    "    text = re_sub(r\"[-+]?[.\\d]*[\\d]+[:,.\\d]*\", \" <number> \")\n",
    "    \n",
    "    # Tag hastags and separate hashtag words:\n",
    "    text = re_sub(r\"#\\w+\", _hashtag)\n",
    "    \n",
    "    # Remove and tag repetitions of question marks, exclamations:\n",
    "    text = re_sub(r\"([!?.]){2,}\", r\"\\1 <repeat> \")\n",
    "    \n",
    "    # Tag elongated sequences of the same letter (3+ occurences), e.g. \"AAAAAAA\"\n",
    "    text = re_sub(r\"\\b(\\S*?)(.)\\2{2,}\\b\", r\"\\1\\2 <elong> \")\n",
    "    \n",
    "    # Put whitespace around punctuation, then remove:\n",
    "    text = re_sub(r\"([a-zA-Z<>()])([{}])\".format(punctuation), r\"\\1 \\2\")\n",
    "    text = re_sub(r\"([{}])([a-zA-Z<>()])\".format(punctuation), r\"\\1 \\2\")\n",
    "    text = re.sub(\"[%s]\" % re.escape(punctuation), \"\", text)\n",
    "\n",
    "    # Tag all caps words:\n",
    "    text = re_sub(r\" ([A-Z]){2,} \", _allcaps)\n",
    "    \n",
    "    return \" \".join(text.lower().split())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fake example with lots of weird features:\n",
    "text = \"#BigFire on #MountEverest #OMG, !! /where are police 'some' punctuation! 3now 44 @police\"\n",
    "tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example from the dataset:\n",
    "example_tweet = train[\"text\"][0]\n",
    "print(\"Original Tweet:\\n\", f\"  {example_tweet}\\n\")\n",
    "print(\"After preprocessing script:\\n\", f\"{tokenize(example_tweet)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying the Word Embeddings\n",
    "\n",
    "The pretrained word embeddings convert each word in a processed tweet (which is in its vocabulary) to a $d$-length vector. Because the Neural Network requires a single vector input, we need a way to convert the multiple word embeddings for a single tweet into one vector. Using a suggestion <a href=\"https://stats.stackexchange.com/a/239071/115143\">here</a>, we try 2 approaches:\n",
    "\n",
    "1. Take the simple average of the vectors for all words in the tweet (resulting in vector of length $d$).\n",
    "2. Concatenate the coordinate-wise minimum and maximum values in each word vector (resulting in vector of length $2d$).\n",
    "\n",
    "In addition to applying the preprocessor, we also add a method to correct common spelling mistakes, which through experimentation improves model performance and allows us to find word embeddings for more than 90% of the input words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from spellchecker import SpellChecker\n",
    "import yaml\n",
    "\n",
    "\n",
    "class WordEmbeddings:\n",
    "    \"\"\"Class to create word embeddings from input tweets, \n",
    "    including text pre-processing.\"\"\"\n",
    "    \n",
    "    def __init__(self, X: pd.Series, embeddings: KeyedVectors, \n",
    "                 processor = tokenize, spellcheck: bool = True):\n",
    "        \"\"\"Takes input tweets, preprocesses them, and converts them \n",
    "        to the 2 different word embedding representations.\n",
    "        \n",
    "        Args:\n",
    "            X: input unprocessed tweets.\n",
    "            embeddings: word embeddings scheme from gensim.\n",
    "            processor: method for preprocessing tweets.\n",
    "            spellcheck: if True, use spellchecker package to correct common\n",
    "                spelling mistakes in input tweets before preprocessing.\n",
    "                \n",
    "        \"\"\"\n",
    "        # Save the raw input as a list:\n",
    "        self.X_raw = X.tolist()\n",
    "        \n",
    "        # Apply the preprocessor function:\n",
    "        X = X.map(processor).tolist()\n",
    "        \n",
    "        # Apply the spelling corrections:\n",
    "        if spellcheck:\n",
    "            corrections = self.correct_spellings([])\n",
    "            corrected = list()\n",
    "            for tweet in X:\n",
    "                corrected.append(\" \".join([corrections.get(w, w) for w in tweet.split()]))\n",
    "            X = corrected\n",
    "\n",
    "        self.X = X\n",
    "        \n",
    "        # Store the word embeddings scheme:\n",
    "        self.embeddings = embeddings\n",
    "        \n",
    "        # Unique vocabulary of the embedding scheme:\n",
    "        self.embeddings_vocab = set(embeddings.key_to_index)\n",
    "        \n",
    "        # Unique vocabulary of the inputs:\n",
    "        self.X_vocab = {w for words in [s.split() for s in X] for w in words}\n",
    "        \n",
    "        # Unknown words in the input:\n",
    "        self.unknown_words = self.X_vocab - self.embeddings_vocab\n",
    "\n",
    "        # Common vocabulary between the input and the embeddings scheme:\n",
    "        self.common_vocab = self.embeddings_vocab & self.X_vocab\n",
    "        self.vocab_coverage = len(self.common_vocab) / len(self.X_vocab)\n",
    "        print(f\"{len(self.X_vocab):} processed input words, \"\\\n",
    "              f\"{len(self.common_vocab):} found in embeddings \"\\\n",
    "              f\"({self.vocab_coverage*100:.2f}%).\")\n",
    "\n",
    "        # Lookup table from words to their embeddings:\n",
    "        self.embeddings_table = pd.DataFrame(embeddings[self.common_vocab], index=self.common_vocab).sort_index()\n",
    "        \n",
    "        # Iterate through input tweets removing unknown words, creating average/min/max embeddings:\n",
    "        embedding_mu, embedding_max, embedding_min, clean_tweets = list(), list(), list(), list()\n",
    "        for i, tweet in enumerate(self.X):\n",
    "            \n",
    "            # Remove unknown words:\n",
    "            words = tweet.split()\n",
    "            known_words = [w for w in words if w in self.common_vocab]\n",
    "            clean_tweets.append(\" \".join(known_words))\n",
    "            \n",
    "            # Calculate mean of all word embeddings in tweet:\n",
    "            mean_array = self.embeddings_table.loc[known_words].mean().values\n",
    "            embedding_mu.append(mean_array)\n",
    "            \n",
    "            # Calculate min/max of all word embeddings in tweet:\n",
    "            max_array = self.embeddings_table.loc[known_words].max().values\n",
    "            embedding_max.append(max_array)\n",
    "            min_array = self.embeddings_table.loc[known_words].min().values\n",
    "            embedding_min.append(min_array)\n",
    "            \n",
    "        # Arrays of final X datasets in word embedding formats:\n",
    "        self.embedding_mu = pd.DataFrame(np.array(embedding_mu))\n",
    "        self.embedding_min = np.array(embedding_min)\n",
    "        self.embedding_max = np.array(embedding_max)\n",
    "        self.embedding_minmax = pd.DataFrame(np.concatenate([self.embedding_min, self.embedding_max], axis=1))\n",
    "        \n",
    "    @property\n",
    "    def embedding_chars(self):\n",
    "        \"\"\"Unique characters in the word embeddings.\"\"\"\n",
    "        return \"\".join(sorted(set([c for w in self.embeddings.index_to_key for c in w])))\n",
    "    \n",
    "    @staticmethod\n",
    "    def correct_spellings(words: list):\n",
    "        \"\"\"Use pyspellchecker to save a dictionary of unknown words\n",
    "        to corrected spellings, or just load presaved corrections. \n",
    "        See:\n",
    "            https://github.com/barrust/pyspellchecker\n",
    "        \"\"\"\n",
    "        fp = os.path.join(os.getcwd(), \"spelling_corrections.yaml\")\n",
    "        if not os.path.exists(fp):\n",
    "            corrections = dict()\n",
    "            with open(fp, \"w\") as stream:\n",
    "                yaml.safe_dump(corrections, stream)\n",
    "        else:\n",
    "            with open(fp, \"r\") as stream:\n",
    "                corrections = yaml.safe_load(stream)\n",
    "            if corrections is None:\n",
    "                corrections = dict()\n",
    "        to_correct = [w for w in words if w not in corrections]\n",
    "        if to_correct:\n",
    "            spell = SpellChecker()\n",
    "            new = {w: spell.correction(w) for w in to_correct}\n",
    "            corrections = {**corrections, **new}\n",
    "            with open(fp, \"w\") as stream:\n",
    "                yaml.safe_dump(corrections, stream)\n",
    "            print(f\"Saved {len(new):} spelling corrections to file.\")\n",
    "        return corrections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the class to apply embeddings to the training & validation data:\n",
    "train_we = WordEmbeddings(train[\"text\"], glove_twitter)\n",
    "validation_we = WordEmbeddings(validation[\"text\"], glove_twitter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network\n",
    "\n",
    "Using TensorFlow to train model with \"early stopping\" to prevent over-fitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class TfModel:\n",
    "    \n",
    "    def __init__(self, X_train: WordEmbeddings, y_train: pd.Series, \n",
    "                 X_val: WordEmbeddings, y_val: pd.Series, \n",
    "                 epochs: int = 500, patience: int = 5):\n",
    "        \"\"\"Implement a TensorFlow model on WordEmbeddings training and \n",
    "        validation data.\"\"\"\n",
    "        \n",
    "        # Store model inputs:\n",
    "        self.X_train, self.y_train = X_train, y_train\n",
    "        self.X_val, self.y_val = X_val, y_val\n",
    "                \n",
    "        # Train separate models for different embeddings schemes:\n",
    "        self.results, self.best_score, self.best_model = dict(), -np.inf, None\n",
    "        for embeddings in (\"embedding_mu\", \"embedding_minmax\"):            \n",
    "            print(f\"Training {embeddings} model\\n\")\n",
    "            \n",
    "            model = tf.keras.Sequential([\n",
    "                tf.keras.layers.Dense(128, activation=\"relu\"),\n",
    "                tf.keras.layers.Dense(64, activation='relu'),\n",
    "                tf.keras.layers.Dense(2)\n",
    "            ])\n",
    "\n",
    "            model.compile(\n",
    "                optimizer=\"adam\",\n",
    "                loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                metrics=[\"accuracy\"]\n",
    "            )\n",
    "\n",
    "            callback = tf.keras.callbacks.EarlyStopping(\n",
    "                monitor=\"val_accuracy\", \n",
    "                mode=\"max\", \n",
    "                min_delta=0.001,\n",
    "                patience=patience, \n",
    "                restore_best_weights=True\n",
    "            )\n",
    "            \n",
    "            X_train_df = getattr(X_train, embeddings)\n",
    "            X_val_df = getattr(X_val, embeddings)\n",
    "            history = model.fit(\n",
    "                x=X_train_df,\n",
    "                y=self.y_train,\n",
    "                batch_size=512,\n",
    "                callbacks=[callback],\n",
    "                verbose=2,\n",
    "                validation_data=(X_val_df, self.y_val),\n",
    "                epochs=epochs,\n",
    "            )\n",
    "\n",
    "            # Use probabilities to make predictions on validation set:\n",
    "            proba_model = tf.keras.Sequential([\n",
    "                model, \n",
    "                tf.keras.layers.Softmax()\n",
    "            ])\n",
    "            train_proba_pred = proba_model.predict(X_train_df)\n",
    "            val_proba_pred = proba_model.predict(X_val_df)\n",
    "            train_pred = np.argmax(train_proba_pred, axis=1)\n",
    "            val_pred = np.argmax(val_proba_pred, axis=1)\n",
    "            \n",
    "            # Store full results:\n",
    "            results_data = {\n",
    "                \"model\": model,\n",
    "                \"history\": history,\n",
    "                \"proba_model\": proba_model,\n",
    "                \"train_proba_pred\": train_proba_pred,\n",
    "                \"val_proba_pred\": val_proba_pred,\n",
    "                \"train_pred\": train_pred,\n",
    "                \"val_pred\": val_pred,\n",
    "            }\n",
    "            self.results[embeddings] = results_data\n",
    "            \n",
    "            # Score the model on overall accuracy on validation set:\n",
    "            accuracy = accuracy_score(self.y_val, val_pred)\n",
    "            if accuracy > self.best_score:\n",
    "                self.best_score = accuracy\n",
    "                self.best_model = embeddings\n",
    "            print()\n",
    "    \n",
    "    @property\n",
    "    def inspect_train(self):\n",
    "        df = pd.DataFrame({\n",
    "            \"raw_tweet\": self.X_train.X_raw,\n",
    "            \"processed_tweet\": self.X_train.X,\n",
    "            \"true_label\": self.y_train.values,\n",
    "            \"prediction\": self.results[self.best_model][\"train_pred\"]\n",
    "        })\n",
    "        return df\n",
    "\n",
    "    @property\n",
    "    def inspect_val(self):\n",
    "        df = pd.DataFrame({\n",
    "            \"raw_tweet\": self.X_val.X_raw,\n",
    "            \"processed_tweet\": self.X_val.X,\n",
    "            \"true_label\": self.y_val.values,\n",
    "            \"prediction\": self.results[self.best_model][\"val_pred\"]\n",
    "        })\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf_nn = TfModel(\n",
    "    train_we, train_labels, validation_we, validation_labels, epochs=500, patience=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Best word embeddings scheme = {tf_nn.best_model}\\nAccuracy = {tf_nn.best_score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall we see that applying the word embeddings and using a Neural Network does not improve performance over the baseline model. Not shown in the code above, we also experimented with removing common stop words from the corpus by applying IDF to all the words in the training tweets, and removing words which were very common across all (disaster and non-disaster) tweets. This did not improve model performance and generally degraded it.\n",
    "\n",
    "As with the other models we inspect some false positives/negatives to look for patterns in the mis-classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_positives = tf_nn.inspect_val[\n",
    "    (tf_nn.inspect_val[\"true_label\"] == 0) & \n",
    "    (tf_nn.inspect_val[\"prediction\"] == 1)\n",
    "]\n",
    "false_negatives = tf_nn.inspect_val[\n",
    "    (tf_nn.inspect_val[\"true_label\"] == 1) & \n",
    "    (tf_nn.inspect_val[\"prediction\"] == 0)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_colwidth\", 500)\n",
    "print(f\"False Positives\")\n",
    "false_positives.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of the false positives contain words that you may expect to confuse the classifier, e.g. `explosion, blast, snowstorm, armaggeddon, collapse` etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"False Negatives\")\n",
    "false_negatives.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again it seems like one of the main issues is that some of the tweets in the training data are mis-labeled, e.g. `103: Skinny Jeans are Hazardous for Your Health! ...` and `94: We're #hiring! ...` do not seem like disaster tweets. To improve performance significantly we may need to get hold of a correctly labeled dataset.\n",
    "\n",
    "As before, we apply the preprocessing steps and use the trained model to predict the scores on the test data, and submit to Kaggle to get the final score of model performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_we = WordEmbeddings(test[\"text\"], glove_twitter)\n",
    "X_test_df = getattr(test_we, tf_nn.best_model)\n",
    "\n",
    "# Get the best performing model, and use it to predict the test data:\n",
    "model = tf_nn.results[tf_nn.best_model][\"model\"]\n",
    "proba_model = tf.keras.Sequential([\n",
    "    model, \n",
    "    tf.keras.layers.Softmax()\n",
    "])\n",
    "test_proba_pred = proba_model.predict(X_test_df)\n",
    "test_pred = np.argmax(test_proba_pred, axis=1)\n",
    "\n",
    "nn_submission = pd.DataFrame({\"id\": test[\"id\"], \"target\": test_pred})\n",
    "nn_submission.to_csv(\"neural_network_test_predictions.csv\", index=False)\n",
    "\n",
    "print(\"Score on Kaggle = 0.80907\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
