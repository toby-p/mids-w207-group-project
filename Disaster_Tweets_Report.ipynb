{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Processing with Disaster Tweets\n",
    "##### Authors: Ruth Ashford, Toby Petty and Li Jin\n",
    "This report is based on a Kaggle competition: https://www.kaggle.com/c/nlp-getting-started/data\n",
    "\n",
    "The aim is to classify tweets as being about a disaster or not. We are given a labeled training dataset along with an unlabeled test dataset. \n",
    "\n",
    "In the report, we perform initial EDA on the data, cleaning and processing of the tweets' text and evaluate the performance of Naive Bayes, Neural Network and Logistic Regression models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (7613, 5)\n",
      "Test:  (3263, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation orders in California</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location  \\\n",
       "0   1     NaN      NaN   \n",
       "1   4     NaN      NaN   \n",
       "2   5     NaN      NaN   \n",
       "3   6     NaN      NaN   \n",
       "4   7     NaN      NaN   \n",
       "\n",
       "                                                                                                                                    text  \\\n",
       "0                                                                  Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all   \n",
       "1                                                                                                 Forest fire near La Ronge Sask. Canada   \n",
       "2  All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected   \n",
       "3                                                                      13,000 people receive #wildfires evacuation orders in California    \n",
       "4                                               Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school    \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_colwidth\", 500)\n",
    "\n",
    "data_fp = os.path.join(os.getcwd(), \"data\")\n",
    "train_fp = os.path.join(data_fp, \"train.csv\")\n",
    "train_full = pd.read_csv(train_fp, encoding=\"utf-8\")\n",
    "test_fp = os.path.join(data_fp, \"test.csv\")\n",
    "test = pd.read_csv(test_fp, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"Train: {train_full.shape}\")\n",
    "print(f\"Test:  {test.shape}\")\n",
    "\n",
    "train_full.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kaggle supplies the test set (without target labels), so we will submit our predictions there to get the ultimate measure of model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory data analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly we check the number of samples that are labelled as disaster (1) and non-disaster (0):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total</th>\n",
       "      <th>Percentage</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4342</td>\n",
       "      <td>57.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3271</td>\n",
       "      <td>43.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Total  Percentage\n",
       "target                   \n",
       "0        4342        57.0\n",
       "1        3271        43.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label_summary = train_full.groupby('target').agg(Total = ('id', 'count'))\n",
    "train_label_summary['Percentage'] = round(train_label_summary['Total'] / train_full.shape[0] * 100, 1)\n",
    "train_label_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows us that we have a fairly even split of disaster and non-disaster labeled samples. This is useful when training our model but tell us that this dataset is not likely to be representative of the general population of tweets, since we know in general that far less than 43% of tweets are about disasters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keywords\n",
    "Next we want to take a look at the keywords and how they relate to the target. The majority of tweets in our training data have a keyword associated with them, there is just 0.2% of samples that don't."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Percentage of tweets without a keyword: 0.2%\n"
     ]
    }
   ],
   "source": [
    "keyword_label_summary = train_full.groupby('keyword').agg(percentage_disaster_occurance = ('target', 'mean')).reset_index()\n",
    "\n",
    "keyword_disaster_count = train_full[train_full['target'] == 1].groupby('keyword').agg(disaster_count = ('target', 'count')).reset_index()\n",
    "keyword_not_disaster_count = train_full[train_full['target'] == 0].groupby('keyword').agg(not_disaster_count = ('target', 'count')).reset_index()\n",
    "\n",
    "keyword_label_summary = keyword_label_summary.merge(keyword_disaster_count, how='left')\n",
    "keyword_label_summary = keyword_label_summary.merge(keyword_not_disaster_count, how='left')\n",
    "\n",
    "print(f' Percentage of tweets without a keyword: {round(1 - train_full.keyword.isna().sum() / train_full.shape[0] * 100 , 2)}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the words that come up most often in disaster tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>percentage_disaster_occurance</th>\n",
       "      <th>disaster_count</th>\n",
       "      <th>not_disaster_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>debris</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>37.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>wreckage</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>39.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>derailment</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>39.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>outbreak</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>39.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>oil%20spill</td>\n",
       "      <td>0.973684</td>\n",
       "      <td>37.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>typhoon</td>\n",
       "      <td>0.973684</td>\n",
       "      <td>37.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>suicide%20bombing</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>suicide%20bomber</td>\n",
       "      <td>0.967742</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>bombing</td>\n",
       "      <td>0.931034</td>\n",
       "      <td>27.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>rescuers</td>\n",
       "      <td>0.914286</td>\n",
       "      <td>32.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>suicide%20bomb</td>\n",
       "      <td>0.914286</td>\n",
       "      <td>32.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>nuclear%20disaster</td>\n",
       "      <td>0.911765</td>\n",
       "      <td>31.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>evacuated</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>32.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>razed</td>\n",
       "      <td>0.885714</td>\n",
       "      <td>31.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>wildfire</td>\n",
       "      <td>0.878788</td>\n",
       "      <td>29.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>wild%20fires</td>\n",
       "      <td>0.870968</td>\n",
       "      <td>27.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>airplane%20accident</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>30.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>mass%20murder</td>\n",
       "      <td>0.848485</td>\n",
       "      <td>28.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>buildings%20on%20fire</td>\n",
       "      <td>0.848485</td>\n",
       "      <td>28.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>forest%20fires</td>\n",
       "      <td>0.843750</td>\n",
       "      <td>27.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   keyword  percentage_disaster_occurance  disaster_count  \\\n",
       "62                  debris                       1.000000            37.0   \n",
       "219               wreckage                       1.000000            39.0   \n",
       "70              derailment                       1.000000            39.0   \n",
       "153               outbreak                       0.975000            39.0   \n",
       "152            oil%20spill                       0.973684            37.0   \n",
       "205                typhoon                       0.973684            37.0   \n",
       "187      suicide%20bombing                       0.969697            32.0   \n",
       "186       suicide%20bomber                       0.967742            30.0   \n",
       "32                 bombing                       0.931034            27.0   \n",
       "166               rescuers                       0.914286            32.0   \n",
       "185         suicide%20bomb                       0.914286            32.0   \n",
       "147     nuclear%20disaster                       0.911765            31.0   \n",
       "96               evacuated                       0.888889            32.0   \n",
       "162                  razed                       0.885714            31.0   \n",
       "214               wildfire                       0.878788            29.0   \n",
       "213           wild%20fires                       0.870968            27.0   \n",
       "3      airplane%20accident                       0.857143            30.0   \n",
       "139          mass%20murder                       0.848485            28.0   \n",
       "35   buildings%20on%20fire                       0.848485            28.0   \n",
       "116         forest%20fires                       0.843750            27.0   \n",
       "\n",
       "     not_disaster_count  \n",
       "62                  NaN  \n",
       "219                 NaN  \n",
       "70                  NaN  \n",
       "153                 1.0  \n",
       "152                 1.0  \n",
       "205                 1.0  \n",
       "187                 1.0  \n",
       "186                 1.0  \n",
       "32                  2.0  \n",
       "166                 3.0  \n",
       "185                 3.0  \n",
       "147                 3.0  \n",
       "96                  4.0  \n",
       "162                 4.0  \n",
       "214                 4.0  \n",
       "213                 4.0  \n",
       "3                   5.0  \n",
       "139                 5.0  \n",
       "35                  5.0  \n",
       "116                 5.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keyword_label_summary.sort_values('percentage_disaster_occurance', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the keywords that come up most often in non-disaster tweets, there are some duplicates such as 'body%20bags' and body%20bag'. There are a lot of words here that could be associated with disasters, such as 'ruin', 'explode' and 'wrecked' but these are also words that could be used in a sentence when not describing a disaster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>percentage_disaster_occurance</th>\n",
       "      <th>disaster_count</th>\n",
       "      <th>not_disaster_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aftershock</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>body%20bags</td>\n",
       "      <td>0.024390</td>\n",
       "      <td>1.0</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>ruin</td>\n",
       "      <td>0.027027</td>\n",
       "      <td>1.0</td>\n",
       "      <td>36.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>blazing</td>\n",
       "      <td>0.029412</td>\n",
       "      <td>1.0</td>\n",
       "      <td>33.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>body%20bag</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>1.0</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>electrocute</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>1.0</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>screaming</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>2.0</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>traumatised</td>\n",
       "      <td>0.057143</td>\n",
       "      <td>2.0</td>\n",
       "      <td>33.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>panicking</td>\n",
       "      <td>0.060606</td>\n",
       "      <td>2.0</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>blew%20up</td>\n",
       "      <td>0.060606</td>\n",
       "      <td>2.0</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>blight</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>2.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>wrecked</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>3.0</td>\n",
       "      <td>36.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>explode</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>3.0</td>\n",
       "      <td>35.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>panic</td>\n",
       "      <td>0.081081</td>\n",
       "      <td>3.0</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>epicentre</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>bloody</td>\n",
       "      <td>0.085714</td>\n",
       "      <td>3.0</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>smoke</td>\n",
       "      <td>0.088235</td>\n",
       "      <td>3.0</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>collide</td>\n",
       "      <td>0.088235</td>\n",
       "      <td>3.0</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>stretcher</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>3.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>drown</td>\n",
       "      <td>0.093750</td>\n",
       "      <td>3.0</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         keyword  percentage_disaster_occurance  disaster_count  \\\n",
       "2     aftershock                       0.000000             NaN   \n",
       "29   body%20bags                       0.024390             1.0   \n",
       "170         ruin                       0.027027             1.0   \n",
       "19       blazing                       0.029412             1.0   \n",
       "27    body%20bag                       0.030303             1.0   \n",
       "88   electrocute                       0.031250             1.0   \n",
       "173    screaming                       0.055556             2.0   \n",
       "201  traumatised                       0.057143             2.0   \n",
       "156    panicking                       0.060606             2.0   \n",
       "21     blew%20up                       0.060606             2.0   \n",
       "22        blight                       0.062500             2.0   \n",
       "220      wrecked                       0.076923             3.0   \n",
       "98       explode                       0.078947             3.0   \n",
       "155        panic                       0.081081             3.0   \n",
       "94     epicentre                       0.083333             1.0   \n",
       "25        bloody                       0.085714             3.0   \n",
       "180        smoke                       0.088235             3.0   \n",
       "48       collide                       0.088235             3.0   \n",
       "183    stretcher                       0.090909             3.0   \n",
       "83         drown                       0.093750             3.0   \n",
       "\n",
       "     not_disaster_count  \n",
       "2                  34.0  \n",
       "29                 40.0  \n",
       "170                36.0  \n",
       "19                 33.0  \n",
       "27                 32.0  \n",
       "88                 31.0  \n",
       "173                34.0  \n",
       "201                33.0  \n",
       "156                31.0  \n",
       "21                 31.0  \n",
       "22                 30.0  \n",
       "220                36.0  \n",
       "98                 35.0  \n",
       "155                34.0  \n",
       "94                 11.0  \n",
       "25                 32.0  \n",
       "180                31.0  \n",
       "48                 31.0  \n",
       "183                30.0  \n",
       "83                 29.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keyword_label_summary.sort_values('percentage_disaster_occurance', ascending=True).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Length of tweets\n",
    "We want to understand if the length of the tweet differs for disaster tweets versus non-disaster tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length of disaster tweets is 108 with standard deviation 29\n",
      "Average length of non-disaster tweets is 96 with standard deviation 36\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(2)\n",
    "fig. tight_layout(pad=3.0)\n",
    "\n",
    "disaster_length = train_full[train_full['target'] == 1]['text'].apply(lambda x: len(x))\n",
    "non_disaster_length = train_full[train_full['target'] == 0]['text'].apply(lambda x: len(x))\n",
    "\n",
    "axs[0].hist(x = disaster_length, color='red', bins=20, range = (0,160))\n",
    "axs[0].set_title('Length distribution of disaster tweets')\n",
    "axs[1].hist(x = non_disaster_length, bins=20, range = (0,160))\n",
    "axs[1].set_title('Length distribution of non-disaster tweets')\n",
    "axs[1].set_xlabel('Tweet length')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(f'Average length of disaster tweets is {round(disaster_length.mean())} '\n",
    "      f'with standard deviation {round(disaster_length.std())}')\n",
    "\n",
    "print(f'Average length of non-disaster tweets is {round(non_disaster_length.mean())} '\n",
    "      f'with standard deviation {round(non_disaster_length.std())}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the disaster tweets seem to be slightly longer, but not by a huge amount, and the overall distributions are similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of words\n",
    "Similarily, we want to understand if there are any key differences in the number of words in the tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length of disaster tweets is 19 with standard deviation 6\n",
      "Average length of non-disaster tweets is 19 with standard deviation 7\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "fig, axs = plt.subplots(2)\n",
    "fig. tight_layout(pad=3.0)\n",
    "\n",
    "disaster_wc = train_full[train_full['target'] == 1]['text'].apply(lambda x: len(word_tokenize(x)))\n",
    "non_disaster_wc = train_full[train_full['target'] == 0]['text'].apply(lambda x: len(word_tokenize(x)))\n",
    "\n",
    "axs[0].hist(x = disaster_wc, color='red', bins=20, range = (0,40))\n",
    "axs[0].set_title('Word count distribution of disaster tweets')\n",
    "axs[1].hist(x = non_disaster_wc, bins=20, range = (0,40))\n",
    "axs[1].set_title('Word count distribution of non-disaster tweets')\n",
    "axs[1].set_xlabel('Tweet word count')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(f'Average length of disaster tweets is {round(disaster_wc.mean())} ' \n",
    "      f'with standard deviation {round(disaster_wc.std())}')\n",
    "\n",
    "print(f'Average length of non-disaster tweets is {round(non_disaster_wc.mean())} '\n",
    "      f'with standard deviation {round(non_disaster_wc.std())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, no real difference here between the distribution of word counts between disaster and non-disaster tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Location\n",
    "\n",
    "Location data is provided for 65% of the samples. The location data seems to range from being as broad as a country, or as specific as the name of a restaurant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of tweets with a location: 66.73%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Birmingham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Est. September 2012 - Bristol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AFRICA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Philadelphia, PA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3337</th>\n",
       "      <td>TN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3338</th>\n",
       "      <td>#NewcastleuponTyne #UK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3339</th>\n",
       "      <td>Vancouver, Canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3340</th>\n",
       "      <td>London</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3341</th>\n",
       "      <td>Lincoln</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3342 rows  1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           location\n",
       "0                               NaN\n",
       "1                        Birmingham\n",
       "2     Est. September 2012 - Bristol\n",
       "3                            AFRICA\n",
       "4                  Philadelphia, PA\n",
       "...                             ...\n",
       "3337                             TN\n",
       "3338         #NewcastleuponTyne #UK\n",
       "3339              Vancouver, Canada\n",
       "3340                        London \n",
       "3341                        Lincoln\n",
       "\n",
       "[3342 rows x 1 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'Percentage of tweets with a location: '\n",
    "      f'{round((1 - train_full[\"location\"].isna().sum() / train_full.shape[0]) * 100 , 2)}%')\n",
    "\n",
    "pd.DataFrame({\"location\": train_full[\"location\"].unique()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline model\n",
    "\n",
    "We use CountVectorizer and BernoulliNB to create a baseline model, and get a benchmark score against the validation set. Note that the target labels are binary outcomes, so they comprise a Bernoulli distribution where  $p \\approx 0.43$. The chosen metric of performance is F1-Score, based on this being the score used to rank submissions on the Kaggle contest leaderboard.\n",
    "\n",
    "### Training and validation data\n",
    "We reserve 20% of the training data set as a validation data set to test iterations of the model against."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data set contains 6090 samples\n",
      "Validation data set contains 1523 samples\n"
     ]
    }
   ],
   "source": [
    "# Shuffle the index in case there are patterns in the order of the training data:\n",
    "np.random.seed(42)\n",
    "shuffled_ix = list(train_full.index)\n",
    "np.random.shuffle(shuffled_ix)\n",
    "cutoff = int(len(shuffled_ix)*0.8)\n",
    "train = train_full.loc[shuffled_ix[:cutoff]].copy()\n",
    "validation = train_full.loc[shuffled_ix[cutoff:]].copy()\n",
    "\n",
    "print(f'Train data set contains {train.shape[0]} samples')\n",
    "print(f'Validation data set contains {validation.shape[0]} samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train[\"text\"]\n",
    "train_labels = train[\"target\"]\n",
    "validation_data = validation[\"text\"]\n",
    "validation_labels = validation[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import plot_confusion_matrix, f1_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessing the baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli distribution, p = 0.430\n",
      "Validation f1 score = 0.784\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "vec = CountVectorizer()\n",
    "vec.fit(train_data)\n",
    "train_vec = vec.transform(train_data)\n",
    "\n",
    "print(f\"Bernoulli distribution, p = {train_labels.mean():.3f}\")\n",
    "\n",
    "model = BernoulliNB()\n",
    "crossval = cross_validate(model, train_vec, train_labels, cv=5, return_estimator=True)\n",
    "mean_score = crossval[\"test_score\"].mean()\n",
    "best_estimator = crossval[\"estimator\"][crossval[\"test_score\"].argmax()]\n",
    "validation_vec = vec.transform(validation_data)\n",
    "pred = best_estimator.predict(validation_vec)\n",
    "validation_score = f1_score(validation_labels, pred, average=\"weighted\")\n",
    "print(f\"Validation f1 score = {validation_score:.3f}\")\n",
    "\n",
    "cm = plot_confusion_matrix(best_estimator, validation_vec, validation_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The baseline gives us an F1-score on the validation data set of 0.784. We can see from the confusion matrix that there are a lot of false negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def false_negatives(labels, preds, original_data, processed_data):\n",
    "    df = pd.DataFrame()\n",
    "    df['original_data'] = original_data\n",
    "    df['processed_data'] = processed_data\n",
    "    df['label'] = labels\n",
    "    df['prediction'] = preds\n",
    "    return df[(df[\"label\"] == 1) & (df[\"prediction\"] == 0)]\n",
    "\n",
    "def false_positives(labels, preds, original_data, processed_data):\n",
    "    df = pd.DataFrame()\n",
    "    df['original_data'] = original_data\n",
    "    df['processed_data'] = processed_data\n",
    "    df['label'] = labels\n",
    "    df['prediction'] = pred\n",
    "    return df[(df[\"label\"] == 0) & (df[\"prediction\"] == 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>processed_data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1279</th>\n",
       "      <td>Burned dog finds new home with young burn victim http://t.co/Pqrjvgvgxg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6883</th>\n",
       "      <td>@PyramidHead76 one good thing came out of watching the film.  Was too traumatised to watch show so started Halt &amp;amp; Catch Fire on Amazon. :D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7138</th>\n",
       "      <td>@MrMikeEaton @Muazimus_Prime hill hill mountain volcano of hell mountain hill hil.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3156</th>\n",
       "      <td>Gonna call up tomorrow with the aul 'emergency dental appointment' excuse just like the whole tooth falling out incident of last year</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5207</th>\n",
       "      <td>@TheEvilOlives It's the closest structure to the hypo centre that wasn't completely obliterated.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3310</th>\n",
       "      <td>Sooo police dispatch said there was a person threatening to shoot up the Walmart on Rutherford &amp;amp; they had to evacuate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3462</th>\n",
       "      <td>oh yeah my ipod almost exploded last night i was using it while charging and shit was sparking akxbskdn almost died</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2161</th>\n",
       "      <td>Bigamist and his 'first' wife are charged in the deaths of his 'second' pregnant wife her child 8 her mothe... http://t.co/rTEuGB5Tnv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>And so it begins.. day one of the snow apocalypse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4524</th>\n",
       "      <td>@Hurricane_Dolce no prob</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                      processed_data\n",
       "1279                                                                         Burned dog finds new home with young burn victim http://t.co/Pqrjvgvgxg\n",
       "6883  @PyramidHead76 one good thing came out of watching the film.  Was too traumatised to watch show so started Halt &amp; Catch Fire on Amazon. :D\n",
       "7138                                                              @MrMikeEaton @Muazimus_Prime hill hill mountain volcano of hell mountain hill hil.\n",
       "3156           Gonna call up tomorrow with the aul 'emergency dental appointment' excuse just like the whole tooth falling out incident of last year\n",
       "5207                                                @TheEvilOlives It's the closest structure to the hypo centre that wasn't completely obliterated.\n",
       "3310                       Sooo police dispatch said there was a person threatening to shoot up the Walmart on Rutherford &amp; they had to evacuate\n",
       "3462                             oh yeah my ipod almost exploded last night i was using it while charging and shit was sparking akxbskdn almost died\n",
       "2161           Bigamist and his 'first' wife are charged in the deaths of his 'second' pregnant wife her child 8 her mothe... http://t.co/rTEuGB5Tnv\n",
       "285                                                                                                And so it begins.. day one of the snow apocalypse\n",
       "4524                                                                                                                        @Hurricane_Dolce no prob"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "false_negatives(validation_labels, pred, validation_data, \n",
    "                validation_data)['processed_data'].to_frame()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>processed_data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3594</th>\n",
       "      <td>The Five Fatal Flaws in the Iran Deal https://t.co/ztfEAd8GId via @YouTube</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5619</th>\n",
       "      <td>Newlyweds feed Syrian refugees at their wedding - CBC News - Latest Canada World Entertainment and Business News http://t.co/QU8S89pVVt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5083</th>\n",
       "      <td>Nuclear deal disaster.\\r\\n\\r\\n#IranDeal #NoNuclearIran #BadIranDeal @JebBush @BarackObama http://t.co/z7phPjtqud</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3057</th>\n",
       "      <td>#Sismo M 1.3 - 1km NNE of The Geysers California: Time2015-08-05 23:40:21 UTC2015-08-05 16:40:21 -07:00 a... http://t.co/x6el3ySYcn #CS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6251</th>\n",
       "      <td>New #photo Oak in a snowstorm http://t.co/JhSCGDA2G8 on the #SouthDowns #Hampshire #Winter #photography #art #tree #treescape #treeporn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341</th>\n",
       "      <td>9 Charts Prove Financial Crisis Part 2 Has BEGUN!: The Financial Armageddon Economic Collapse Blog tracks tren... http://t.co/vHCXTvCINr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6946</th>\n",
       "      <td>@freefromwolves GodsLove &amp;amp; #thankU brother Danny for RT of NEW VIDEO http://t.co/cybKsXHF7d The Coming Apocalyptic US Earthquake &amp;amp; Tsunami</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7100</th>\n",
       "      <td>Violent Forces Radio: Now Playing Agony - Storm of the apocalypse\\r\\nTuneIn Player @ http://t.co/XsSgEdSbH4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3056</th>\n",
       "      <td>#USGS M 1.4 - 4km E of Interlaken California: Time2015-08-06 00:52:25 UTC2015-08-05 17:52:25 -07:00 at ep... http://t.co/zqrcptLrUM #SM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>.: .: .: .: .: .: .: .: .: .: .: .: .: .: .: .: .: .: .: .: .: RT DrAyesha4: #IndiaKoMunTorJawabDo\\r\\n\\r\\nIndian Army ki_ http://t.co/WJLJq3yA4g</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                          processed_data\n",
       "3594                                                                          The Five Fatal Flaws in the Iran Deal https://t.co/ztfEAd8GId via @YouTube\n",
       "5619             Newlyweds feed Syrian refugees at their wedding - CBC News - Latest Canada World Entertainment and Business News http://t.co/QU8S89pVVt\n",
       "5083                                    Nuclear deal disaster.\\r\\n\\r\\n#IranDeal #NoNuclearIran #BadIranDeal @JebBush @BarackObama http://t.co/z7phPjtqud\n",
       "3057             #Sismo M 1.3 - 1km NNE of The Geysers California: Time2015-08-05 23:40:21 UTC2015-08-05 16:40:21 -07:00 a... http://t.co/x6el3ySYcn #CS\n",
       "6251             New #photo Oak in a snowstorm http://t.co/JhSCGDA2G8 on the #SouthDowns #Hampshire #Winter #photography #art #tree #treescape #treeporn\n",
       "341             9 Charts Prove Financial Crisis Part 2 Has BEGUN!: The Financial Armageddon Economic Collapse Blog tracks tren... http://t.co/vHCXTvCINr\n",
       "6946  @freefromwolves GodsLove &amp; #thankU brother Danny for RT of NEW VIDEO http://t.co/cybKsXHF7d The Coming Apocalyptic US Earthquake &amp; Tsunami\n",
       "7100                                         Violent Forces Radio: Now Playing Agony - Storm of the apocalypse\\r\\nTuneIn Player @ http://t.co/XsSgEdSbH4\n",
       "3056             #USGS M 1.4 - 4km E of Interlaken California: Time2015-08-06 00:52:25 UTC2015-08-05 17:52:25 -07:00 at ep... http://t.co/zqrcptLrUM #SM\n",
       "362   .: .: .: .: .: .: .: .: .: .: .: .: .: .: .: .: .: .: .: .: .: RT DrAyesha4: #IndiaKoMunTorJawabDo\\r\\n\\r\\nIndian Army ki_ http://t.co/WJLJq3yA4g"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "false_positives(validation_labels, pred, validation_data, \n",
    "                validation_data)['processed_data'].to_frame()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the false positives there are a lot of hyperlinks, punctuation and @ symbols that can be cleaned up.\n",
    "\n",
    "### Assessing using the test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vec = vec.transform(test[\"text\"])\n",
    "test_preds = best_estimator.predict(test_vec)\n",
    "nb_submission = pd.DataFrame({\"id\": test.id, \"target\": test_preds})\n",
    "nb_submission.to_csv(\"baseline_test_predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This submission gave us a Kaggle score of 0.79282"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1 - Naive Bayes\n",
    "\n",
    "We will clean the data up and use stemming and lemmatization to see if we can improve upon the baseline Naive Bayes model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\toby\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\toby\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "stemmer = PorterStemmer()\n",
    "lemmer = WordNetLemmatizer()\n",
    "\n",
    "def stem_sentence(text):\n",
    "    token_words=word_tokenize(text)\n",
    "    stem_sentence=[]\n",
    "    for word in token_words:\n",
    "        stem_sentence.append(stemmer.stem(word))\n",
    "        stem_sentence.append(\" \")\n",
    "    return \"\".join(stem_sentence)\n",
    "\n",
    "def lem_sentence(text):\n",
    "    token_words=word_tokenize(text)\n",
    "    stem_sentence=[]\n",
    "    for word in token_words:\n",
    "        stem_sentence.append(lemmer.lemmatize(word, pos='n'))\n",
    "        stem_sentence.append(\" \")\n",
    "    return \"\".join(stem_sentence)\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text)\n",
    "    text = re.sub(r'http?:\\/\\/.*[\\r\\n]*', '', text)\n",
    "    text = re.sub(r\"(?:\\@)\\w+\", ' ', text)\n",
    "    text = re.sub(r'[:\"#$%&\\*+,-/:;<=>@\\\\^_`{|}~?!]+',' ',text)\n",
    "    text = re.sub(r\"[']+\",'',text)\n",
    "    \n",
    "    text=text.replace(r'&amp;?',r'and')\n",
    "    text=text.replace(r'&lt;',r'<')\n",
    "    text=text.replace(r'&gt;',r'>')\n",
    "    \n",
    "    text=text.encode(\"ascii\",errors=\"ignore\").decode()\n",
    "\n",
    "    return text\n",
    "\n",
    "def clean_and_lem(text):\n",
    "    return lem_sentence(clean_text(text))\n",
    "\n",
    "def clean_and_stem(text):\n",
    "    return stem_sentence(clean_text(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean tweets text\n",
    "We clean the text to try to remove anything that adds noise, for example hyperlinks. \n",
    "In addition to this we've added Laplace smoothing to the model (alpha parameter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation f1 score = 0.797\n"
     ]
    }
   ],
   "source": [
    "vec = CountVectorizer(analyzer=\"word\", preprocessor=clean_text)\n",
    "vec.fit(train_data)\n",
    "train_vec = vec.transform(train_data)\n",
    "\n",
    "model = BernoulliNB(alpha=0.85)\n",
    "crossval = cross_validate(model, train_vec, train_labels, cv=5, return_estimator=True)\n",
    "mean_score = crossval[\"test_score\"].mean()\n",
    "best_estimator = crossval[\"estimator\"][crossval[\"test_score\"].argmax()]\n",
    "validation_vec = vec.transform(validation_data)\n",
    "pred = best_estimator.predict(validation_vec)\n",
    "validation_score = f1_score(validation_labels, pred, average=\"weighted\")\n",
    "print(f\"Validation f1 score = {validation_score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning the data has improved the F1-score on the validation data set to 0.797"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "We use the Porter stemmer to normalize words in the tweets. A stemming algorithm reduces a word to its root, for example retrieval, retrieved, retrieves reduce to the stem retrieve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation f1 score = 0.796\n"
     ]
    }
   ],
   "source": [
    "vec = CountVectorizer(analyzer=\"word\", preprocessor=clean_and_stem)\n",
    "vec.fit(train_data)\n",
    "train_vec = vec.transform(train_data)\n",
    "\n",
    "model = BernoulliNB(alpha=0.85)\n",
    "crossval = cross_validate(model, train_vec, train_labels, cv=5, return_estimator=True)\n",
    "mean_score = crossval[\"test_score\"].mean()\n",
    "best_estimator = crossval[\"estimator\"][crossval[\"test_score\"].argmax()]\n",
    "validation_vec = vec.transform(validation_data)\n",
    "pred = best_estimator.predict(validation_vec)\n",
    "validation_score = f1_score(validation_labels, pred, average=\"weighted\")\n",
    "print(f\"Validation f1 score = {validation_score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying stemming to the data after cleaning it has reduced the F1-score slightly, this may be due to overstemming where two words of different meaning are reduced to the same stem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "Lemmatization is similar to stemming but it converts a word to its base form while also taking into account the context of the words. For example using lemmatization, 'caring' gets converted to 'care', whereas when using a stemmer, it will likely get converted to 'car'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy on 5-fold cross validation = 0.802\n",
      "Validation f1 score = 0.802\n"
     ]
    }
   ],
   "source": [
    "vec = CountVectorizer(analyzer=\"word\", preprocessor=clean_and_lem)\n",
    "vec.fit(train_data)\n",
    "train_vec = vec.transform(train_data)\n",
    "\n",
    "model = BernoulliNB(alpha=0.75)\n",
    "crossval = cross_validate(model, train_vec, train_labels, cv=5, return_estimator=True)\n",
    "mean_score = crossval[\"test_score\"].mean()\n",
    "print(f\"Mean accuracy on 5-fold cross validation = {mean_score:.3f}\")\n",
    "best_estimator = crossval[\"estimator\"][crossval[\"test_score\"].argmax()]\n",
    "validation_vec = vec.transform(validation_data)\n",
    "pred = best_estimator.predict(validation_vec)\n",
    "validation_score = f1_score(validation_labels, pred, average=\"weighted\")\n",
    "print(f\"Validation f1 score = {validation_score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatizing has been more successful and has increased the validation F1-score to 0.802"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term Frequency - Inverse Document Frequency \n",
    "TF-IDF is based on the same bag of words technique as Count Vectorizer but it considers how important the word is. It does this by determining how rare the word is by dividing the number of times it appears in the document by the number of times it appears in any document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy on 5-fold cross validation = 0.803\n",
      "Validation f1 score = 0.801\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vec = TfidfVectorizer(preprocessor=clean_and_lem, encoding=\"utf-8\",\n",
    "                      ngram_range=(1,1), max_features=11000, norm=\"l2\")\n",
    "vec.fit(train_data)\n",
    "train_vec = vec.transform(train_data)\n",
    "\n",
    "model = BernoulliNB(alpha=0.75)\n",
    "crossval = cross_validate(model, train_vec, train_labels, cv=5, return_estimator=True)\n",
    "mean_score = crossval[\"test_score\"].mean()\n",
    "print(f\"Mean accuracy on 5-fold cross validation = {mean_score:.3f}\")\n",
    "best_estimator = crossval[\"estimator\"][crossval[\"test_score\"].argmax()]\n",
    "validation_vec = vec.transform(validation_data)\n",
    "pred = best_estimator.predict(validation_vec)\n",
    "validation_score = f1_score(validation_labels, pred, average=\"weighted\")\n",
    "print(f\"Validation f1 score = {validation_score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_data</th>\n",
       "      <th>processed_data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1279</th>\n",
       "      <td>Burned dog finds new home with young burn victim http://t.co/Pqrjvgvgxg</td>\n",
       "      <td>burned dog find new home with young burn victim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6883</th>\n",
       "      <td>@PyramidHead76 one good thing came out of watching the film.  Was too traumatised to watch show so started Halt &amp;amp; Catch Fire on Amazon. :D</td>\n",
       "      <td>one good thing came out of watching the film wa too traumatised to watch show so started halt amp catch fire on amazon d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7138</th>\n",
       "      <td>@MrMikeEaton @Muazimus_Prime hill hill mountain volcano of hell mountain hill hil.</td>\n",
       "      <td>hill hill mountain volcano of hell mountain hill hil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3156</th>\n",
       "      <td>Gonna call up tomorrow with the aul 'emergency dental appointment' excuse just like the whole tooth falling out incident of last year</td>\n",
       "      <td>gon na call up tomorrow with the aul emergency dental appointment excuse just like the whole tooth falling out incident of last year</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5207</th>\n",
       "      <td>@TheEvilOlives It's the closest structure to the hypo centre that wasn't completely obliterated.</td>\n",
       "      <td>it the closest structure to the hypo centre that wasnt completely obliterated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3462</th>\n",
       "      <td>oh yeah my ipod almost exploded last night i was using it while charging and shit was sparking akxbskdn almost died</td>\n",
       "      <td>oh yeah my ipod almost exploded last night i wa using it while charging and shit wa sparking akxbskdn almost died</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2161</th>\n",
       "      <td>Bigamist and his 'first' wife are charged in the deaths of his 'second' pregnant wife her child 8 her mothe... http://t.co/rTEuGB5Tnv</td>\n",
       "      <td>bigamist and his first wife are charged in the death of his second pregnant wife her child 8 her mothe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>And so it begins.. day one of the snow apocalypse</td>\n",
       "      <td>and so it begin day one of the snow apocalypse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4524</th>\n",
       "      <td>@Hurricane_Dolce no prob</td>\n",
       "      <td>no prob</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6771</th>\n",
       "      <td>Maybe that's what happens when a tornado meets a volcano</td>\n",
       "      <td>maybe thats what happens when a tornado meet a volcano</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2717</th>\n",
       "      <td>Zayn Malik &amp;amp; Perrie Edwards End Engagement: Shes Devastated http://t.co/GedOxSPpL9 http://t.co/ACZRUOrYtD</td>\n",
       "      <td>zayn malik amp perrie edward end engagement shes devastated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3212</th>\n",
       "      <td>We're #hiring! Click to apply: RN II/EMERGENCY SERVICES/FT/7P-7A - http://t.co/NV3Uxv9IMX #Nursing #Houston TX http://t.co/ej30IhrEA9</td>\n",
       "      <td>were hiring click to apply rn ii emergency service ft 7p 7a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6707</th>\n",
       "      <td>@HaydnExists so glad i saved them all at once then didnt want you stealing my thunder :P</td>\n",
       "      <td>so glad i saved them all at once then didnt want you stealing my thunder p</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4120</th>\n",
       "      <td>Summer heat drives bobcats to Calgary backyards ~ 35 http://t.co/TmzXopVs94 http://t.co/W192Wkog1M</td>\n",
       "      <td>summer heat drive bobcat to calgary backyard 35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5298</th>\n",
       "      <td>@SenateMajLdr let's try to do our best to prevent another outbreak of violence by talking to each other both the people and the politics</td>\n",
       "      <td>let try to do our best to prevent another outbreak of violence by talking to each other both the people and the politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>http://t.co/FueRk0gWui Twelve feared killed in Pakistani air ambulance helicopter crash http://t.co/Mv7GgGlmVc</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5031</th>\n",
       "      <td>@Lolly_Knickers It's a mudslide. \\r\\nIt's like chewing on a rubber tyre.\\r\\nAnd with those I'm DONE.\\r\\n#vaginaorcake #GBBO</td>\n",
       "      <td>it a mudslide it like chewing on a rubber tyre and with those im done vaginaorcake gbbo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2872</th>\n",
       "      <td>The Drought Is Real ??????</td>\n",
       "      <td>the drought is real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3030</th>\n",
       "      <td>#3Novices : Renison mine sees seismic event http://t.co/2i4EOGGO5j A small earthquake at Tasmania's Renison tin project has created a tem_</td>\n",
       "      <td>3novices renison mine see seismic event</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645</th>\n",
       "      <td>@durrellb Prices here are insane. Our dollar has collapsed against the US and it's punishing us. Thanks for the info.</td>\n",
       "      <td>price here are insane our dollar ha collapsed against the u and it punishing u thanks for the info</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                       original_data  \\\n",
       "1279                                                                         Burned dog finds new home with young burn victim http://t.co/Pqrjvgvgxg   \n",
       "6883  @PyramidHead76 one good thing came out of watching the film.  Was too traumatised to watch show so started Halt &amp; Catch Fire on Amazon. :D   \n",
       "7138                                                              @MrMikeEaton @Muazimus_Prime hill hill mountain volcano of hell mountain hill hil.   \n",
       "3156           Gonna call up tomorrow with the aul 'emergency dental appointment' excuse just like the whole tooth falling out incident of last year   \n",
       "5207                                                @TheEvilOlives It's the closest structure to the hypo centre that wasn't completely obliterated.   \n",
       "3462                             oh yeah my ipod almost exploded last night i was using it while charging and shit was sparking akxbskdn almost died   \n",
       "2161           Bigamist and his 'first' wife are charged in the deaths of his 'second' pregnant wife her child 8 her mothe... http://t.co/rTEuGB5Tnv   \n",
       "285                                                                                                And so it begins.. day one of the snow apocalypse   \n",
       "4524                                                                                                                        @Hurricane_Dolce no prob   \n",
       "6771                                                                                        Maybe that's what happens when a tornado meets a volcano   \n",
       "2717                          Zayn Malik &amp; Perrie Edwards End Engagement: Shes Devastated http://t.co/GedOxSPpL9 http://t.co/ACZRUOrYtD   \n",
       "3212           We're #hiring! Click to apply: RN II/EMERGENCY SERVICES/FT/7P-7A - http://t.co/NV3Uxv9IMX #Nursing #Houston TX http://t.co/ej30IhrEA9   \n",
       "6707                                                     @HaydnExists so glad i saved them all at once then didnt want you stealing my thunder :P   \n",
       "4120                                              Summer heat drives bobcats to Calgary backyards ~ 35 http://t.co/TmzXopVs94 http://t.co/W192Wkog1M   \n",
       "5298        @SenateMajLdr let's try to do our best to prevent another outbreak of violence by talking to each other both the people and the politics   \n",
       "201                                   http://t.co/FueRk0gWui Twelve feared killed in Pakistani air ambulance helicopter crash http://t.co/Mv7GgGlmVc   \n",
       "5031                     @Lolly_Knickers It's a mudslide. \\r\\nIt's like chewing on a rubber tyre.\\r\\nAnd with those I'm DONE.\\r\\n#vaginaorcake #GBBO   \n",
       "2872                                                                                                                      The Drought Is Real ??????   \n",
       "3030    #3Novices : Renison mine sees seismic event http://t.co/2i4EOGGO5j A small earthquake at Tasmania's Renison tin project has created a tem_   \n",
       "1645                           @durrellb Prices here are insane. Our dollar has collapsed against the US and it's punishing us. Thanks for the info.   \n",
       "\n",
       "                                                                                                                             processed_data  \n",
       "1279                                                                                       burned dog find new home with young burn victim   \n",
       "6883              one good thing came out of watching the film wa too traumatised to watch show so started halt amp catch fire on amazon d   \n",
       "7138                                                                                  hill hill mountain volcano of hell mountain hill hil   \n",
       "3156  gon na call up tomorrow with the aul emergency dental appointment excuse just like the whole tooth falling out incident of last year   \n",
       "5207                                                         it the closest structure to the hypo centre that wasnt completely obliterated   \n",
       "3462                     oh yeah my ipod almost exploded last night i wa using it while charging and shit wa sparking akxbskdn almost died   \n",
       "2161                                bigamist and his first wife are charged in the death of his second pregnant wife her child 8 her mothe   \n",
       "285                                                                                         and so it begin day one of the snow apocalypse   \n",
       "4524                                                                                                                               no prob   \n",
       "6771                                                                                maybe thats what happens when a tornado meet a volcano   \n",
       "2717                                                                           zayn malik amp perrie edward end engagement shes devastated   \n",
       "3212                                                                           were hiring click to apply rn ii emergency service ft 7p 7a   \n",
       "6707                                                            so glad i saved them all at once then didnt want you stealing my thunder p   \n",
       "4120                                                                                       summer heat drive bobcat to calgary backyard 35   \n",
       "5298              let try to do our best to prevent another outbreak of violence by talking to each other both the people and the politics   \n",
       "201                                                                                                                                          \n",
       "5031                                               it a mudslide it like chewing on a rubber tyre and with those im done vaginaorcake gbbo   \n",
       "2872                                                                                                                   the drought is real   \n",
       "3030                                                                                               3novices renison mine see seismic event   \n",
       "1645                                    price here are insane our dollar ha collapsed against the u and it punishing u thanks for the info   "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "false_negatives(validation_labels, pred, validation_data, \n",
    "                validation_data.apply(lambda x: clean_and_lem(x)))[['original_data', 'processed_data']][:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's interesting to see a lot of the tweets here that were originally labelled as disasters don't seem to be about disasters, so when examining these it seems like our model may be correctly classifying them as non-disaster tweets. We'll get predictions for the test data set provided by Kaggle using this model and see how it performs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessing using the test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vec = vec.transform(test[\"text\"])\n",
    "test_preds = best_estimator.predict(test_vec)\n",
    "nb_submission = pd.DataFrame({\"id\": test.id, \"target\": test_preds})\n",
    "nb_submission.to_csv(\"naive_bayes_test_predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This submission gave us a Kaggle score of 0.79313"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2 - Logistic Regression\n",
    "\n",
    "In this section, we will introduce our second model using Logistic Regression, and analysis of the results. This section will use the same preprocessing and TF-IDF Vectorizer as the previous model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model and Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy on 5-fold cross validation = 0.807\n",
      "Validation f1 score = 0.782\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(train_vec, train_labels)\n",
    "pred = model.predict(validation_vec)\n",
    "\n",
    "crossval = cross_validate(model, train_vec, train_labels, cv=5, return_estimator=True)\n",
    "mean_score = crossval[\"test_score\"].mean()\n",
    "print(f\"Mean accuracy on 5-fold cross validation = {mean_score:.3f}\")\n",
    "best_estimator = crossval[\"estimator\"][crossval[\"test_score\"].argmax()]\n",
    "validation_vec = vec.transform(validation_data)\n",
    "pred = best_estimator.predict(validation_vec)\n",
    "validation_score = f1_score(validation_labels, pred, average=\"weighted\")\n",
    "print(f\"Validation f1 score = {validation_score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The score is a little worse than Model 1. Next we will do some error analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Analysis\n",
    "\n",
    "####  Top Coefficients\n",
    "\n",
    "First we look at the largest coefficients to see if they make sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table of top 50 features and their coefficients\n",
      "                positive\n",
      "(fire,)         3.810866\n",
      "(in,)           3.343749\n",
      "(hiroshima,)    3.085050\n",
      "(train,)        2.900931\n",
      "(california,)   2.779010\n",
      "(storm,)        2.644719\n",
      "(wildfire,)     2.558687\n",
      "(bombing,)      2.469046\n",
      "(flood,)        2.427287\n",
      "(suicide,)      2.357004\n",
      "(disaster,)     2.262189\n",
      "(near,)         2.096496\n",
      "(police,)       2.037451\n",
      "(building,)     1.990621\n",
      "(murder,)       1.962556\n",
      "(at,)           1.849177\n",
      "(over,)         1.842410\n",
      "(typhoon,)      1.841772\n",
      "(mass,)         1.828538\n",
      "(after,)        1.816829\n",
      "(war,)          1.811453\n",
      "(japan,)        1.800284\n",
      "(derailment,)   1.797760\n",
      "(drought,)      1.795589\n",
      "(warning,)      1.792510\n",
      "(crash,)        1.783022\n",
      "(terrorist,)    1.726204\n",
      "(earthquake,)   1.721908\n",
      "(collapse,)     1.709789\n",
      "(debris,)       1.703589\n",
      "(car,)          1.698958\n",
      "(killed,)       1.696351\n",
      "(massacre,)     1.684230\n",
      "(tornado,)      1.667455\n",
      "(mh370,)        1.660156\n",
      "(nuclear,)      1.648894\n",
      "(during,)       1.638216\n",
      "(home,)         1.633712\n",
      "(migrant,)      1.629718\n",
      "(severe,)       1.592537\n",
      "(casualty,)     1.548728\n",
      "(of,)           1.538940\n",
      "(were,)         1.531217\n",
      "(atomic,)       1.502200\n",
      "(evacuation,)   1.489263\n",
      "(spill,)        1.486331\n",
      "(bridge,)       1.480041\n",
      "(forest,)       1.465655\n",
      "(legionnaire,)  1.464819\n",
      "(area,)         1.461219\n"
     ]
    }
   ],
   "source": [
    "feature_index = np.argsort(-model.coef_)[:, :100].flatten()\n",
    "\n",
    "vocabulary_df = pd.DataFrame({\n",
    "    'text': vec.vocabulary_.keys(),\n",
    "    'feature': vec.vocabulary_.values()\n",
    "}).set_index('feature')\n",
    "\n",
    "df = pd.DataFrame(model.coef_[:, feature_index]).T\n",
    "df.index = vocabulary_df.loc[feature_index]\n",
    "df.columns = ['positive']\n",
    "\n",
    "print(\"Table of top 50 features and their coefficients\")\n",
    "print(df.iloc[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At a glance the coefficients are a mixture of \n",
    "* indicator of a disaster (wildfire, hailstorm, earthquake, ...) \n",
    "* words that can be related to a disaster (catastrophic, police, severe, ...)\n",
    "* Neutral word (airport, coach, gas, near, ...)\n",
    "\n",
    "which seem to make sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### False Negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_data</th>\n",
       "      <th>processed_data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1279</th>\n",
       "      <td>Burned dog finds new home with young burn victim http://t.co/Pqrjvgvgxg</td>\n",
       "      <td>burned dog find new home with young burn victim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6883</th>\n",
       "      <td>@PyramidHead76 one good thing came out of watching the film.  Was too traumatised to watch show so started Halt &amp;amp; Catch Fire on Amazon. :D</td>\n",
       "      <td>one good thing came out of watching the film wa too traumatised to watch show so started halt amp catch fire on amazon d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7138</th>\n",
       "      <td>@MrMikeEaton @Muazimus_Prime hill hill mountain volcano of hell mountain hill hil.</td>\n",
       "      <td>hill hill mountain volcano of hell mountain hill hil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3156</th>\n",
       "      <td>Gonna call up tomorrow with the aul 'emergency dental appointment' excuse just like the whole tooth falling out incident of last year</td>\n",
       "      <td>gon na call up tomorrow with the aul emergency dental appointment excuse just like the whole tooth falling out incident of last year</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5207</th>\n",
       "      <td>@TheEvilOlives It's the closest structure to the hypo centre that wasn't completely obliterated.</td>\n",
       "      <td>it the closest structure to the hypo centre that wasnt completely obliterated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3462</th>\n",
       "      <td>oh yeah my ipod almost exploded last night i was using it while charging and shit was sparking akxbskdn almost died</td>\n",
       "      <td>oh yeah my ipod almost exploded last night i wa using it while charging and shit wa sparking akxbskdn almost died</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4895</th>\n",
       "      <td>Headed to the massacre \\r\\nBodies arriving everyday \\r\\nWhat were those shells you heard \\r\\nPicking the bones up along the way</td>\n",
       "      <td>headed to the massacre body arriving everyday what were those shell you heard picking the bone up along the way</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2161</th>\n",
       "      <td>Bigamist and his 'first' wife are charged in the deaths of his 'second' pregnant wife her child 8 her mothe... http://t.co/rTEuGB5Tnv</td>\n",
       "      <td>bigamist and his first wife are charged in the death of his second pregnant wife her child 8 her mothe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>And so it begins.. day one of the snow apocalypse</td>\n",
       "      <td>and so it begin day one of the snow apocalypse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4524</th>\n",
       "      <td>@Hurricane_Dolce no prob</td>\n",
       "      <td>no prob</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6771</th>\n",
       "      <td>Maybe that's what happens when a tornado meets a volcano</td>\n",
       "      <td>maybe thats what happens when a tornado meet a volcano</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2717</th>\n",
       "      <td>Zayn Malik &amp;amp; Perrie Edwards End Engagement: Shes Devastated http://t.co/GedOxSPpL9 http://t.co/ACZRUOrYtD</td>\n",
       "      <td>zayn malik amp perrie edward end engagement shes devastated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3212</th>\n",
       "      <td>We're #hiring! Click to apply: RN II/EMERGENCY SERVICES/FT/7P-7A - http://t.co/NV3Uxv9IMX #Nursing #Houston TX http://t.co/ej30IhrEA9</td>\n",
       "      <td>were hiring click to apply rn ii emergency service ft 7p 7a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6707</th>\n",
       "      <td>@HaydnExists so glad i saved them all at once then didnt want you stealing my thunder :P</td>\n",
       "      <td>so glad i saved them all at once then didnt want you stealing my thunder p</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4120</th>\n",
       "      <td>Summer heat drives bobcats to Calgary backyards ~ 35 http://t.co/TmzXopVs94 http://t.co/W192Wkog1M</td>\n",
       "      <td>summer heat drive bobcat to calgary backyard 35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5298</th>\n",
       "      <td>@SenateMajLdr let's try to do our best to prevent another outbreak of violence by talking to each other both the people and the politics</td>\n",
       "      <td>let try to do our best to prevent another outbreak of violence by talking to each other both the people and the politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>http://t.co/FueRk0gWui Twelve feared killed in Pakistani air ambulance helicopter crash http://t.co/Mv7GgGlmVc</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3502</th>\n",
       "      <td>You Are Invited to The Expo Explosion Summer Event 2015! \\r\\nWHEN: August 14th Friday 2015\\r\\nWHERE: Ben E Keith... http://t.co/yh4R7Ug21a</td>\n",
       "      <td>you are invited to the expo explosion summer event 2015 when august 14th friday 2015 where ben e keith</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5031</th>\n",
       "      <td>@Lolly_Knickers It's a mudslide. \\r\\nIt's like chewing on a rubber tyre.\\r\\nAnd with those I'm DONE.\\r\\n#vaginaorcake #GBBO</td>\n",
       "      <td>it a mudslide it like chewing on a rubber tyre and with those im done vaginaorcake gbbo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2872</th>\n",
       "      <td>The Drought Is Real ??????</td>\n",
       "      <td>the drought is real</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                       original_data  \\\n",
       "1279                                                                         Burned dog finds new home with young burn victim http://t.co/Pqrjvgvgxg   \n",
       "6883  @PyramidHead76 one good thing came out of watching the film.  Was too traumatised to watch show so started Halt &amp; Catch Fire on Amazon. :D   \n",
       "7138                                                              @MrMikeEaton @Muazimus_Prime hill hill mountain volcano of hell mountain hill hil.   \n",
       "3156           Gonna call up tomorrow with the aul 'emergency dental appointment' excuse just like the whole tooth falling out incident of last year   \n",
       "5207                                                @TheEvilOlives It's the closest structure to the hypo centre that wasn't completely obliterated.   \n",
       "3462                             oh yeah my ipod almost exploded last night i was using it while charging and shit was sparking akxbskdn almost died   \n",
       "4895                 Headed to the massacre \\r\\nBodies arriving everyday \\r\\nWhat were those shells you heard \\r\\nPicking the bones up along the way   \n",
       "2161           Bigamist and his 'first' wife are charged in the deaths of his 'second' pregnant wife her child 8 her mothe... http://t.co/rTEuGB5Tnv   \n",
       "285                                                                                                And so it begins.. day one of the snow apocalypse   \n",
       "4524                                                                                                                        @Hurricane_Dolce no prob   \n",
       "6771                                                                                        Maybe that's what happens when a tornado meets a volcano   \n",
       "2717                          Zayn Malik &amp; Perrie Edwards End Engagement: Shes Devastated http://t.co/GedOxSPpL9 http://t.co/ACZRUOrYtD   \n",
       "3212           We're #hiring! Click to apply: RN II/EMERGENCY SERVICES/FT/7P-7A - http://t.co/NV3Uxv9IMX #Nursing #Houston TX http://t.co/ej30IhrEA9   \n",
       "6707                                                     @HaydnExists so glad i saved them all at once then didnt want you stealing my thunder :P   \n",
       "4120                                              Summer heat drives bobcats to Calgary backyards ~ 35 http://t.co/TmzXopVs94 http://t.co/W192Wkog1M   \n",
       "5298        @SenateMajLdr let's try to do our best to prevent another outbreak of violence by talking to each other both the people and the politics   \n",
       "201                                   http://t.co/FueRk0gWui Twelve feared killed in Pakistani air ambulance helicopter crash http://t.co/Mv7GgGlmVc   \n",
       "3502      You Are Invited to The Expo Explosion Summer Event 2015! \\r\\nWHEN: August 14th Friday 2015\\r\\nWHERE: Ben E Keith... http://t.co/yh4R7Ug21a   \n",
       "5031                     @Lolly_Knickers It's a mudslide. \\r\\nIt's like chewing on a rubber tyre.\\r\\nAnd with those I'm DONE.\\r\\n#vaginaorcake #GBBO   \n",
       "2872                                                                                                                      The Drought Is Real ??????   \n",
       "\n",
       "                                                                                                                             processed_data  \n",
       "1279                                                                                       burned dog find new home with young burn victim   \n",
       "6883              one good thing came out of watching the film wa too traumatised to watch show so started halt amp catch fire on amazon d   \n",
       "7138                                                                                  hill hill mountain volcano of hell mountain hill hil   \n",
       "3156  gon na call up tomorrow with the aul emergency dental appointment excuse just like the whole tooth falling out incident of last year   \n",
       "5207                                                         it the closest structure to the hypo centre that wasnt completely obliterated   \n",
       "3462                     oh yeah my ipod almost exploded last night i wa using it while charging and shit wa sparking akxbskdn almost died   \n",
       "4895                       headed to the massacre body arriving everyday what were those shell you heard picking the bone up along the way   \n",
       "2161                                bigamist and his first wife are charged in the death of his second pregnant wife her child 8 her mothe   \n",
       "285                                                                                         and so it begin day one of the snow apocalypse   \n",
       "4524                                                                                                                               no prob   \n",
       "6771                                                                                maybe thats what happens when a tornado meet a volcano   \n",
       "2717                                                                           zayn malik amp perrie edward end engagement shes devastated   \n",
       "3212                                                                           were hiring click to apply rn ii emergency service ft 7p 7a   \n",
       "6707                                                            so glad i saved them all at once then didnt want you stealing my thunder p   \n",
       "4120                                                                                       summer heat drive bobcat to calgary backyard 35   \n",
       "5298              let try to do our best to prevent another outbreak of violence by talking to each other both the people and the politics   \n",
       "201                                                                                                                                          \n",
       "3502                                you are invited to the expo explosion summer event 2015 when august 14th friday 2015 where ben e keith   \n",
       "5031                                               it a mudslide it like chewing on a rubber tyre and with those im done vaginaorcake gbbo   \n",
       "2872                                                                                                                   the drought is real   "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "false_negatives(validation_labels, pred, validation_data, \n",
    "                validation_data.apply(lambda x: clean_and_lem(x)))[['original_data', 'processed_data']][:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at false negatives, it appears that some of the test labels are wrong. For example, these tweets are clearly not disasters but are labeled as so:\n",
    "\n",
    "* but if you build an army of 100 dogs and their leader is a lion all dogs will fight like a lion\n",
    "* watch these super strong magnets destroy everyday objects\n",
    "* check out my lava lamp dude   \n",
    "* ...\n",
    "\n",
    "These wrong labels can explain why recall for positives is quite low."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### False positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_data</th>\n",
       "      <th>processed_data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3190</th>\n",
       "      <td>Practice your families fire escape plan so everyone knows what to do in case of an emergency.</td>\n",
       "      <td>practice your family fire escape plan so everyone know what to do in case of an emergency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3594</th>\n",
       "      <td>The Five Fatal Flaws in the Iran Deal https://t.co/ztfEAd8GId via @YouTube</td>\n",
       "      <td>the five fatal flaw in the iran deal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1308</th>\n",
       "      <td>RT @HuffPostComedy: We should build a wall that keeps Burning Man attendees from coming home http://t.co/xwVW1sft4I http://t.co/j7HUKhWmal</td>\n",
       "      <td>rt we should build a wall that keep burning man attendee from coming home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6097</th>\n",
       "      <td>?that horrible sinking feeling when youve been at home on your phone for a while and you realise its been on 3G this whole time</td>\n",
       "      <td>that horrible sinking feeling when youve been at home on your phone for a while and you realise it been on 3g this whole time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5619</th>\n",
       "      <td>Newlyweds feed Syrian refugees at their wedding - CBC News - Latest Canada World Entertainment and Business News http://t.co/QU8S89pVVt</td>\n",
       "      <td>newlywed feed syrian refugee at their wedding cbc news latest canada world entertainment and business news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7159</th>\n",
       "      <td>Camping in a war zone with roving raccoons toughens city slicker http://t.co/oJuS08yZrq</td>\n",
       "      <td>camping in a war zone with roving raccoon toughens city slicker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6103</th>\n",
       "      <td>that horrible sinking feeling when youve been at home on your phone for a while and you realise its been on 3G this whole time</td>\n",
       "      <td>that horrible sinking feeling when youve been at home on your phone for a while and you realise it been on 3g this whole time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4873</th>\n",
       "      <td>@BenignoVito @LibertyBell1000 HILLARYMASS MURDERER.</td>\n",
       "      <td>hillarymass murderer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5083</th>\n",
       "      <td>Nuclear deal disaster.\\r\\n\\r\\n#IranDeal #NoNuclearIran #BadIranDeal @JebBush @BarackObama http://t.co/z7phPjtqud</td>\n",
       "      <td>nuclear deal disaster irandeal nonucleariran badirandeal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3057</th>\n",
       "      <td>#Sismo M 1.3 - 1km NNE of The Geysers California: Time2015-08-05 23:40:21 UTC2015-08-05 16:40:21 -07:00 a... http://t.co/x6el3ySYcn #CS</td>\n",
       "      <td>sismo m 1 3 1km nne of the geyser california time2015 08 05 23 40 21 utc2015 08 05 16 40 21 07 00 a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6251</th>\n",
       "      <td>New #photo Oak in a snowstorm http://t.co/JhSCGDA2G8 on the #SouthDowns #Hampshire #Winter #photography #art #tree #treescape #treeporn</td>\n",
       "      <td>new photo oak in a snowstorm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6245</th>\n",
       "      <td>'Cooler than Freddie Jackson sippin' a milkshake in a snowstorm'</td>\n",
       "      <td>cooler than freddie jackson sippin a milkshake in a snowstorm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7100</th>\n",
       "      <td>Violent Forces Radio: Now Playing Agony - Storm of the apocalypse\\r\\nTuneIn Player @ http://t.co/XsSgEdSbH4</td>\n",
       "      <td>violent force radio now playing agony storm of the apocalypse tunein player</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3797</th>\n",
       "      <td>Trident 90225 Chevy fire Truck w/ Pumper  IMS Fire Dept. Red HO 1:87  plastic http://t.co/FAQNFUpeGn http://t.co/mQfOfKXtyh</td>\n",
       "      <td>trident 90225 chevy fire truck w pumper ims fire dept red ho 1 87 plastic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3056</th>\n",
       "      <td>#USGS M 1.4 - 4km E of Interlaken California: Time2015-08-06 00:52:25 UTC2015-08-05 17:52:25 -07:00 at ep... http://t.co/zqrcptLrUM #SM</td>\n",
       "      <td>usgs m 1 4 4km e of interlaken california time2015 08 06 00 52 25 utc2015 08 05 17 52 25 07 00 at ep</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3876</th>\n",
       "      <td>The advantages apropos of in flames favorable regard mississauga ontario: pWHvGwax</td>\n",
       "      <td>the advantage apropos of in flame favorable regard mississauga ontario pwhvgwax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>.: .: .: .: .: .: .: .: .: .: .: .: .: .: .: .: .: .: .: .: .: RT DrAyesha4: #IndiaKoMunTorJawabDo\\r\\n\\r\\nIndian Army ki_ http://t.co/WJLJq3yA4g</td>\n",
       "      <td>rt drayesha4 indiakomuntorjawabdo indian army ki</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5788</th>\n",
       "      <td>@CloydRivers there were plenty of black people rioting when tOSU won the championship as well.</td>\n",
       "      <td>there were plenty of black people rioting when tosu won the championship a well</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3687</th>\n",
       "      <td>Kosciusko police investigating pedestrian fatality hit by a train Thursday http://t.co/m5djLLxoZP</td>\n",
       "      <td>kosciusko police investigating pedestrian fatality hit by a train thursday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5142</th>\n",
       "      <td>A little filming inside a Nuclear Reactor at #Chernobyl @SonyProUSA @LumixUSA @DJIGlobal @ProfBrianCox @RT_America https://t.co/2GLjhvEAD9</td>\n",
       "      <td>a little filming inside a nuclear reactor at chernobyl</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                           original_data  \\\n",
       "3190                                                       Practice your families fire escape plan so everyone knows what to do in case of an emergency.   \n",
       "3594                                                                          The Five Fatal Flaws in the Iran Deal https://t.co/ztfEAd8GId via @YouTube   \n",
       "1308          RT @HuffPostComedy: We should build a wall that keeps Burning Man attendees from coming home http://t.co/xwVW1sft4I http://t.co/j7HUKhWmal   \n",
       "6097                  ?that horrible sinking feeling when youve been at home on your phone for a while and you realise its been on 3G this whole time   \n",
       "5619             Newlyweds feed Syrian refugees at their wedding - CBC News - Latest Canada World Entertainment and Business News http://t.co/QU8S89pVVt   \n",
       "7159                                                             Camping in a war zone with roving raccoons toughens city slicker http://t.co/oJuS08yZrq   \n",
       "6103                   that horrible sinking feeling when youve been at home on your phone for a while and you realise its been on 3G this whole time   \n",
       "4873                                                                                                 @BenignoVito @LibertyBell1000 HILLARYMASS MURDERER.   \n",
       "5083                                    Nuclear deal disaster.\\r\\n\\r\\n#IranDeal #NoNuclearIran #BadIranDeal @JebBush @BarackObama http://t.co/z7phPjtqud   \n",
       "3057             #Sismo M 1.3 - 1km NNE of The Geysers California: Time2015-08-05 23:40:21 UTC2015-08-05 16:40:21 -07:00 a... http://t.co/x6el3ySYcn #CS   \n",
       "6251             New #photo Oak in a snowstorm http://t.co/JhSCGDA2G8 on the #SouthDowns #Hampshire #Winter #photography #art #tree #treescape #treeporn   \n",
       "6245                                                                                    'Cooler than Freddie Jackson sippin' a milkshake in a snowstorm'   \n",
       "7100                                         Violent Forces Radio: Now Playing Agony - Storm of the apocalypse\\r\\nTuneIn Player @ http://t.co/XsSgEdSbH4   \n",
       "3797                         Trident 90225 Chevy fire Truck w/ Pumper  IMS Fire Dept. Red HO 1:87  plastic http://t.co/FAQNFUpeGn http://t.co/mQfOfKXtyh   \n",
       "3056             #USGS M 1.4 - 4km E of Interlaken California: Time2015-08-06 00:52:25 UTC2015-08-05 17:52:25 -07:00 at ep... http://t.co/zqrcptLrUM #SM   \n",
       "3876                                                                  The advantages apropos of in flames favorable regard mississauga ontario: pWHvGwax   \n",
       "362   .: .: .: .: .: .: .: .: .: .: .: .: .: .: .: .: .: .: .: .: .: RT DrAyesha4: #IndiaKoMunTorJawabDo\\r\\n\\r\\nIndian Army ki_ http://t.co/WJLJq3yA4g   \n",
       "5788                                                      @CloydRivers there were plenty of black people rioting when tOSU won the championship as well.   \n",
       "3687                                                   Kosciusko police investigating pedestrian fatality hit by a train Thursday http://t.co/m5djLLxoZP   \n",
       "5142          A little filming inside a Nuclear Reactor at #Chernobyl @SonyProUSA @LumixUSA @DJIGlobal @ProfBrianCox @RT_America https://t.co/2GLjhvEAD9   \n",
       "\n",
       "                                                                                                                      processed_data  \n",
       "3190                                      practice your family fire escape plan so everyone know what to do in case of an emergency   \n",
       "3594                                                                                           the five fatal flaw in the iran deal   \n",
       "1308                                                      rt we should build a wall that keep burning man attendee from coming home   \n",
       "6097  that horrible sinking feeling when youve been at home on your phone for a while and you realise it been on 3g this whole time   \n",
       "5619                     newlywed feed syrian refugee at their wedding cbc news latest canada world entertainment and business news   \n",
       "7159                                                                camping in a war zone with roving raccoon toughens city slicker   \n",
       "6103  that horrible sinking feeling when youve been at home on your phone for a while and you realise it been on 3g this whole time   \n",
       "4873                                                                                                           hillarymass murderer   \n",
       "5083                                                                       nuclear deal disaster irandeal nonucleariran badirandeal   \n",
       "3057                            sismo m 1 3 1km nne of the geyser california time2015 08 05 23 40 21 utc2015 08 05 16 40 21 07 00 a   \n",
       "6251                                                                                                   new photo oak in a snowstorm   \n",
       "6245                                                                  cooler than freddie jackson sippin a milkshake in a snowstorm   \n",
       "7100                                                    violent force radio now playing agony storm of the apocalypse tunein player   \n",
       "3797                                                      trident 90225 chevy fire truck w pumper ims fire dept red ho 1 87 plastic   \n",
       "3056                           usgs m 1 4 4km e of interlaken california time2015 08 06 00 52 25 utc2015 08 05 17 52 25 07 00 at ep   \n",
       "3876                                                the advantage apropos of in flame favorable regard mississauga ontario pwhvgwax   \n",
       "362                                                                                rt drayesha4 indiakomuntorjawabdo indian army ki   \n",
       "5788                                                there were plenty of black people rioting when tosu won the championship a well   \n",
       "3687                                                     kosciusko police investigating pedestrian fatality hit by a train thursday   \n",
       "5142                                                                         a little filming inside a nuclear reactor at chernobyl   "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "false_positives(validation_labels, pred, validation_data, \n",
    "                validation_data.apply(lambda x: clean_and_lem(x)))[['original_data', 'processed_data']][:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at false positives, some of the tweets are very hard for this model to detect, e.g:\n",
    "\n",
    "* battlefield 4 funny moments   dukes of hazard undercover soldier mav t  (video game)\n",
    "* shes a suicide bomb (metaphor)\n",
    "\n",
    "Some of the false positives are possibily due to the bags of word approach itself:\n",
    "\n",
    "* advice from noah  dont go running in a thunderstorm\n",
    "\n",
    "Some of the false positives might get better using a bigram approach:\n",
    "* the government is concerned about the population explosion and the population is concerned about the government explosion    joe moore (population explosion)\n",
    "* pizza drought is over i just couldnt anymore (pizza drought)\n",
    "\n",
    "For example, it may be able to detect that the word \"explosion\" in combination with the word \"population\" is not highly predictive of a disaster tweet, and similarly for \"pizza drought\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigram\n",
    "We try to use bigrams instead of unigrams and see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy on 5-fold cross validation = 0.705\n",
      "Validation f1 score = 0.709\n"
     ]
    }
   ],
   "source": [
    "vec_2 = TfidfVectorizer(\n",
    "    preprocessor=clean_and_lem, encoding=\"utf-8\",\n",
    "    ngram_range=(2, 2),\n",
    "    norm=\"l2\"\n",
    ")\n",
    "train_vec_2 = vec_2.fit_transform(train_data)\n",
    "validation_vec_2 = vec_2.transform(validation_data)\n",
    "\n",
    "model_2 = LogisticRegression()\n",
    "model_2.fit(train_vec_2, train_labels)\n",
    "pred_2 = model_2.predict(validation_vec_2)\n",
    "\n",
    "crossval_2 = cross_validate(model_2, train_vec_2, train_labels, cv=5, return_estimator=True)\n",
    "mean_score_2 = crossval_2[\"test_score\"].mean()\n",
    "print(f\"Mean accuracy on 5-fold cross validation = {mean_score_2:.3f}\")\n",
    "best_estimator_2 = crossval_2[\"estimator\"][crossval_2[\"test_score\"].argmax()]\n",
    "pred_2 = best_estimator_2.predict(validation_vec_2)\n",
    "validation_score = f1_score(validation_labels, pred_2, average=\"weighted\")\n",
    "print(f\"Validation f1 score = {validation_score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, using bigrams results in worse performance. Therefore we will stick to unigrams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessing using the test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vec = vec.transform(test['text'])\n",
    "test_preds = best_estimator.predict(test_vec)\n",
    "nb_submission = pd.DataFrame({'id': test.id, 'target': test_preds})\n",
    "nb_submission.to_csv('logistic_regression_test_predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us a final score of 0.78945."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3 - Neural Network\n",
    "\n",
    "The final approach we try is to use a different representation of text data as word embeddings, and to train a Neural Network model to classify the tweets based on numeric vector inputs created from these word embeddings.\n",
    "\n",
    "### GloVe Word Embeddings\n",
    "\n",
    "The word embeddings scheme chosen is a pretrained GloVe model with ~1.2m word vocabulary, trained on ~2 billion tweets. It is hoped that by using a word embeddings scheme trained on tweets, it will perform well on our dataset which is also scraped from Twitter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\toby\\toby-local\\venvs\\mids-w207-group-project\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-trained vectors based on 2B tweets, 27B tokens, 1.2M vocab, uncased (https://nlp.stanford.edu/projects/glove/).\n"
     ]
    }
   ],
   "source": [
    "from gensim import downloader  # Library which supplies the word embeddings.\n",
    "\n",
    "# Load the pretrained word embeddings (this takes a while):\n",
    "glove_twitter = downloader.load(\"glove-twitter-200\")\n",
    "info = downloader.info()\n",
    "print(info[\"models\"][\"glove-twitter-200\"][\"description\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Processing\n",
    "\n",
    "The creators of this word embeddings scheme also released a <a href=\"https://nlp.stanford.edu/projects/glove/preprocess-twitter.rb\">Ruby script</a> to apply the same text transformations to tweet data that they used when creating the embeddings. A user on Kaggle converted this script to <a href=\"https://www.kaggle.com/amackcrane/python-version-of-glove-twitter-preprocess-script\">Python</a>, which we adapted below with some additional changes to improve the number of input words which can be embedded. Some of these changes include:\n",
    "\n",
    "1. Removing most punctuation entirely. The original Kaggle script leaves a lot of punctation untouched, so that for example the quoted word `'something'` would be included as a separate feature to the unquoted word `something`. We make the assumption that such differences in punctuation of features are unlikely to be significant in identifying disaster tweets.\n",
    "2. Fixing a couple of bugs with the way whitespace was added around certain punctuation marks, such as `.,`\n",
    "3. Add spaces around inserted tags such as ` <number> `\n",
    "\n",
    "Making the above changes to the preprocessor we increase the number of input words which are found in the embeddings vocabulary from ~75% to nearly 90%.\n",
    "\n",
    "One interesting decision in the preprocessing script is to remove tagged twitter usernames (i.e. strings starting with `@`), which we expect could provide some useful information in trying to predict the nature of the tweets. For example, a tweet tagging a local fire department might be much more likely to be a disaster related tweet. However since we are using a pretrained embeddings scheme we don't have the option to include usernames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import string\n",
    "import sys\n",
    "import regex as re\n",
    "\n",
    "FLAGS = re.MULTILINE | re.DOTALL\n",
    "\n",
    "\n",
    "punctuation = string.punctuation.replace(\"#\", \"\")\n",
    "punctuation = punctuation.replace(\"<\", \"\")\n",
    "punctuation = punctuation.replace(\">\", \"\")\n",
    "\n",
    "\n",
    "def _hashtag(text):\n",
    "    \"\"\"Parse hashtags in tweets.\n",
    "    \n",
    "    For hashtags with words in title case, splits into separate words, e.g.\n",
    "        #BigFire -> <hashtag> Big Fire \n",
    "        \n",
    "    For all caps hashtags converts to lowercase, e.g.\n",
    "        #OMG -> <hashtag> omg\n",
    "    \"\"\"\n",
    "    text = text.group()\n",
    "    hashtag_body = text[1:]\n",
    "    if hashtag_body.isupper():\n",
    "        result = f\" <hashtag> {hashtag_body.lower()} <allcaps> \"\n",
    "    else:\n",
    "        result = \" \".join([\" <hashtag> \"] + re.split(r\"(?=[A-Z])\", hashtag_body, flags=FLAGS))\n",
    "    return result\n",
    "\n",
    "\n",
    "def _allcaps(text):\n",
    "    \"\"\"Convert all-caps words to lowercase and add allcaps tag.\"\"\"\n",
    "    text = text.group()\n",
    "    return text.lower() + \" <allcaps> \"\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"Preprocessor for tweets.\"\"\"\n",
    "    def re_sub(pattern, repl):\n",
    "        return re.sub(pattern, repl, text, flags=FLAGS)\n",
    "\n",
    "    # Remove URLS:\n",
    "    text = re_sub(r\"https?:\\/\\/\\S+\\b|www\\.(\\w+\\.)+\\S*\", \" <url> \")\n",
    "    \n",
    "    # Remove usernames:\n",
    "    text = re_sub(r\"@\\w+\", \" <user> \")\n",
    "    \n",
    "    # Tag smiley faces :-) sad faces :-( etc.\n",
    "    eyes, nose = r\"[8:=;]\", r\"['`\\-]?\"\n",
    "    text = re_sub(r\"{}{}[)dD]+|[)dD]+{}{}\".format(eyes, nose, nose, eyes), \" <smile> \")\n",
    "    text = re_sub(r\"{}{}p+\".format(eyes, nose), \" <lolface> \")\n",
    "    text = re_sub(r\"{}{}\\(+|\\)+{}{}\".format(eyes, nose, nose, eyes), \" <sadface> \")\n",
    "    text = re_sub(r\"{}{}[\\/|l*]\".format(eyes, nose), \" <neutralface> \")\n",
    "    text = re_sub(r\"<3\",\" <heart> \")\n",
    "\n",
    "    # Add whitespace around /\n",
    "    text = re_sub(r\"/\",\" / \")\n",
    "    \n",
    "    # Tag numbers:\n",
    "    text = re_sub(r\"[-+]?[.\\d]*[\\d]+[:,.\\d]*\", \" <number> \")\n",
    "    \n",
    "    # Tag hastags and separate hashtag words:\n",
    "    text = re_sub(r\"#\\w+\", _hashtag)\n",
    "    \n",
    "    # Remove and tag repetitions of question marks, exclamations:\n",
    "    text = re_sub(r\"([!?.]){2,}\", r\"\\1 <repeat> \")\n",
    "\n",
    "    # Tag elongated sequences of the same letter (3+ occurences), e.g. \"AAAAAAA\"\n",
    "    text = re_sub(r\"\\b(\\S*?)(\\S)\\2{2,}\\b\", r\"\\1\\2 <elong> \")\n",
    "    \n",
    "    # Put whitespace around punctuation, then remove:\n",
    "    text = re_sub(r\"([a-zA-Z<>()])([{}])\".format(punctuation), r\"\\1 \\2\")\n",
    "    text = re_sub(r\"([{}])([a-zA-Z<>()])\".format(punctuation), r\"\\1 \\2\")\n",
    "    text = re.sub(\"[%s]\" % re.escape(punctuation), \"\", text)\n",
    "\n",
    "    # Tag all caps words:\n",
    "    text = re_sub(r\" ([A-Z]){2,} \", _allcaps)\n",
    "    \n",
    "    return \" \".join(text.lower().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Tweet:\n",
      "   #BigFire on #MountEverest #OMG, !! /where are @police 'some' punctuation! AAAAA\n",
      "\n",
      "After preprocessing script:\n",
      "   <hashtag> big fire on <hashtag> mount everest <hashtag> omg <allcaps> <repeat> where are <user> some punctuation a <elong>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fake example with lots of weird features:\n",
    "text = \"#BigFire on #MountEverest #OMG, !! /where are @police 'some' punctuation! AAAAA\"\n",
    "print(\"Original Tweet:\\n\", f\"  {text}\\n\")\n",
    "print(\"After preprocessing script:\\n\", f\"  {tokenize(text)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Tweet:\n",
      "   Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all\n",
      "\n",
      "After preprocessing script:\n",
      "   our deeds are the reason of this <hashtag> earthquake may allah <allcaps> forgive us all\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example from the dataset:\n",
    "example_tweet = train[\"text\"][0]\n",
    "print(\"Original Tweet:\\n\", f\"  {example_tweet}\\n\")\n",
    "print(\"After preprocessing script:\\n\", f\"  {tokenize(example_tweet)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying the Word Embeddings\n",
    "\n",
    "The pretrained word embeddings convert each word in a processed tweet (which is in its vocabulary) to a $d$-length vector (where for this particular embeddings scheme $d=200$). Because the Neural Network requires a single vector input, we need a way to convert the multiple word embeddings for a single tweet into one vector. Using a suggestion <a href=\"https://stats.stackexchange.com/a/239071/115143\">here</a>, we try 2 approaches:\n",
    "\n",
    "1. Take the simple mean of the vectors for all words in the tweet (resulting in vector of length $d$).\n",
    "2. Concatenate the coordinate-wise minimum and maximum values in each word vector (resulting in vector of length $2d$).\n",
    "\n",
    "In addition to applying the preprocessor, we also add a method to correct common spelling mistakes, which through experimentation improves model performance and allows us to find word embeddings for more than 90% of the input words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from spellchecker import SpellChecker\n",
    "import yaml\n",
    "\n",
    "\n",
    "class WordEmbeddings:\n",
    "    \"\"\"Class to create word embeddings from input tweets, \n",
    "    including text pre-processing.\"\"\"\n",
    "    \n",
    "    def __init__(self, X: pd.Series, embeddings: KeyedVectors, \n",
    "                 processor = tokenize, spellcheck: bool = True):\n",
    "        \"\"\"Takes input tweets, preprocesses them, and converts them \n",
    "        to the 2 different word embedding representations.\n",
    "        \n",
    "        Args:\n",
    "            X: input unprocessed tweets.\n",
    "            embeddings: word embeddings scheme from gensim.\n",
    "            processor: method for preprocessing tweets.\n",
    "            spellcheck: if True, use spellchecker package to correct common\n",
    "                spelling mistakes in input tweets before preprocessing.\n",
    "        \"\"\"\n",
    "        # Save the raw input as a list:\n",
    "        self.X_raw = X.tolist()\n",
    "        \n",
    "        # Apply the preprocessor function:\n",
    "        X = X.map(processor).tolist()\n",
    "        \n",
    "        # Apply the spelling corrections:\n",
    "        if spellcheck:\n",
    "            corrections = self.correct_spellings([])\n",
    "            corrected = list()\n",
    "            for tweet in X:\n",
    "                corrected.append(\" \".join([corrections.get(w, w) for w in tweet.split()]))\n",
    "            X = corrected\n",
    "\n",
    "        self.X = X\n",
    "        \n",
    "        # Store the word embeddings scheme:\n",
    "        self.embeddings = embeddings\n",
    "        \n",
    "        # Unique vocabulary of the embedding scheme:\n",
    "        self.embeddings_vocab = set(embeddings.key_to_index)\n",
    "        \n",
    "        # Unique vocabulary of the inputs:\n",
    "        self.X_vocab = {w for words in [s.split() for s in X] for w in words}\n",
    "        \n",
    "        # Unknown words in the input:\n",
    "        self.unknown_words = self.X_vocab - self.embeddings_vocab\n",
    "\n",
    "        # Common vocabulary between the input and the embeddings scheme:\n",
    "        self.common_vocab = self.embeddings_vocab & self.X_vocab\n",
    "        self.vocab_coverage = len(self.common_vocab) / len(self.X_vocab)\n",
    "        print(f\"{len(self.X_vocab):} processed input words, \"\\\n",
    "              f\"{len(self.common_vocab):} found in embeddings \"\\\n",
    "              f\"({self.vocab_coverage*100:.2f}%).\")\n",
    "\n",
    "        # Lookup table from words to their embeddings:\n",
    "        self.embeddings_table = pd.DataFrame(embeddings[self.common_vocab], index=self.common_vocab).sort_index()\n",
    "        \n",
    "        # Iterate through input tweets removing unknown words, creating average/min/max embeddings:\n",
    "        embedding_mu, embedding_max, embedding_min, clean_tweets = list(), list(), list(), list()\n",
    "        for i, tweet in enumerate(self.X):\n",
    "            \n",
    "            # Remove unknown words:\n",
    "            words = tweet.split()\n",
    "            known_words = [w for w in words if w in self.common_vocab]\n",
    "            clean_tweets.append(\" \".join(known_words))\n",
    "            \n",
    "            # Calculate mean of all word embeddings in tweet:\n",
    "            mean_array = self.embeddings_table.loc[known_words].mean().values\n",
    "            embedding_mu.append(mean_array)\n",
    "            \n",
    "            # Calculate min/max of all word embeddings in tweet:\n",
    "            max_array = self.embeddings_table.loc[known_words].max().values\n",
    "            embedding_max.append(max_array)\n",
    "            min_array = self.embeddings_table.loc[known_words].min().values\n",
    "            embedding_min.append(min_array)\n",
    "            \n",
    "        # Arrays of final X datasets in word embedding formats:\n",
    "        self.embedding_mu = pd.DataFrame(np.array(embedding_mu))\n",
    "        self.embedding_min = np.array(embedding_min)\n",
    "        self.embedding_max = np.array(embedding_max)\n",
    "        self.embedding_minmax = pd.DataFrame(np.concatenate([self.embedding_min, self.embedding_max], axis=1))\n",
    "        \n",
    "    @property\n",
    "    def embedding_chars(self):\n",
    "        \"\"\"Unique characters in the word embeddings.\"\"\"\n",
    "        return \"\".join(sorted(set([c for w in self.embeddings.index_to_key for c in w])))\n",
    "    \n",
    "    @staticmethod\n",
    "    def correct_spellings(words: list):\n",
    "        \"\"\"Use pyspellchecker to save a dictionary of unknown words\n",
    "        to corrected spellings, or just load presaved corrections. \n",
    "        See:\n",
    "            https://github.com/barrust/pyspellchecker\n",
    "        \"\"\"\n",
    "        fp = os.path.join(os.getcwd(), \"spelling_corrections.yaml\")\n",
    "        if not os.path.exists(fp):\n",
    "            corrections = dict()\n",
    "            with open(fp, \"w\") as stream:\n",
    "                yaml.safe_dump(corrections, stream)\n",
    "        else:\n",
    "            with open(fp, \"r\") as stream:\n",
    "                corrections = yaml.safe_load(stream)\n",
    "            if corrections is None:\n",
    "                corrections = dict()\n",
    "        to_correct = [w for w in words if w not in corrections]\n",
    "        if to_correct:\n",
    "            spell = SpellChecker()\n",
    "            new = {w: spell.correction(w) for w in to_correct}\n",
    "            corrections = {**corrections, **new}\n",
    "            with open(fp, \"w\") as stream:\n",
    "                yaml.safe_dump(corrections, stream)\n",
    "            print(f\"Saved {len(new):} spelling corrections to file.\")\n",
    "        return corrections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12145 processed input words, 11404 found in embeddings (93.90%).\n",
      "5507 processed input words, 5141 found in embeddings (93.35%).\n"
     ]
    }
   ],
   "source": [
    "# Use the class to apply embeddings to the training & validation data:\n",
    "train_we = WordEmbeddings(train[\"text\"], glove_twitter)\n",
    "validation_we = WordEmbeddings(validation[\"text\"], glove_twitter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network\n",
    "\n",
    "We use TensorFlow to train a model with \"early stopping\" to prevent over-fitting. Note that we optimize the model using accuracy instead of F1-score because it is built in to Tensorflow so is much easier to use. Since there is not a huge class imbalance in the training data accuracy should still work well as a metric to optimize. We still classify the best embeddings scheme as the one which produces the highest F1-score on the validation dataset.\n",
    "\n",
    "The model architecture contains 2 hidden layers, one with 128 units and one with 64 units. This was selected based on being a common architecture in various online code samples, but we also tested more complex architectures with more layers and more units, and less complex architectures with 1 hidden layer and fewer units. The resulting performance differences were not particularly significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class TfModel:\n",
    "    \n",
    "    def __init__(self, X_train: WordEmbeddings, y_train: pd.Series, \n",
    "                 X_val: WordEmbeddings, y_val: pd.Series, \n",
    "                 epochs: int = 500, patience: int = 5):\n",
    "        \"\"\"Implement a TensorFlow model on WordEmbeddings training and \n",
    "        validation data.\"\"\"\n",
    "        \n",
    "        # Store model inputs:\n",
    "        self.X_train, self.y_train = X_train, y_train\n",
    "        self.X_val, self.y_val = X_val, y_val\n",
    "                \n",
    "        # Train separate models for different embeddings schemes:\n",
    "        self.results, self.best_score, self.best_model = dict(), -np.inf, None\n",
    "        for embeddings in (\"embedding_mu\", \"embedding_minmax\"):            \n",
    "            print(f\"Training {embeddings} model\\n\")\n",
    "            \n",
    "            model = tf.keras.Sequential([\n",
    "                tf.keras.layers.Dense(128, activation=\"relu\"),\n",
    "                tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "                tf.keras.layers.Dense(2)\n",
    "            ])\n",
    "\n",
    "            model.compile(\n",
    "                optimizer=\"adam\",\n",
    "                loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                metrics=[\"accuracy\"]\n",
    "            )\n",
    "\n",
    "            callback = tf.keras.callbacks.EarlyStopping(\n",
    "                monitor=\"val_accuracy\", \n",
    "                mode=\"max\", \n",
    "                min_delta=0.001,\n",
    "                patience=patience, \n",
    "                restore_best_weights=True\n",
    "            )\n",
    "            \n",
    "            X_train_df = getattr(X_train, embeddings)\n",
    "            X_val_df = getattr(X_val, embeddings)\n",
    "            history = model.fit(\n",
    "                x=X_train_df,\n",
    "                y=self.y_train,\n",
    "                batch_size=512,\n",
    "                callbacks=[callback],\n",
    "                verbose=2,\n",
    "                validation_data=(X_val_df, self.y_val),\n",
    "                epochs=epochs,\n",
    "            )\n",
    "\n",
    "            # Use probabilities to make predictions on validation set:\n",
    "            proba_model = tf.keras.Sequential([\n",
    "                model, \n",
    "                tf.keras.layers.Softmax()\n",
    "            ])\n",
    "            train_proba_pred = proba_model.predict(X_train_df)\n",
    "            val_proba_pred = proba_model.predict(X_val_df)\n",
    "            train_pred = np.argmax(train_proba_pred, axis=1)\n",
    "            val_pred = np.argmax(val_proba_pred, axis=1)\n",
    "            \n",
    "            # Store full results:\n",
    "            results_data = {\n",
    "                \"model\": model,\n",
    "                \"history\": history,\n",
    "                \"proba_model\": proba_model,\n",
    "                \"train_proba_pred\": train_proba_pred,\n",
    "                \"val_proba_pred\": val_proba_pred,\n",
    "                \"train_pred\": train_pred,\n",
    "                \"val_pred\": val_pred,\n",
    "            }\n",
    "            self.results[embeddings] = results_data\n",
    "            \n",
    "            # Score the model on overall F1-score on validation set:\n",
    "            f1 = f1_score(self.y_val, val_pred, average=\"weighted\")\n",
    "            if f1 > self.best_score:\n",
    "                self.best_score = f1\n",
    "                self.best_model = embeddings\n",
    "            print()\n",
    "    \n",
    "    @property\n",
    "    def inspect_train(self):\n",
    "        df = pd.DataFrame({\n",
    "            \"raw_tweet\": self.X_train.X_raw,\n",
    "            \"processed_tweet\": self.X_train.X,\n",
    "            \"true_label\": self.y_train.values,\n",
    "            \"prediction\": self.results[self.best_model][\"train_pred\"]\n",
    "        })\n",
    "        return df\n",
    "\n",
    "    @property\n",
    "    def inspect_val(self):\n",
    "        df = pd.DataFrame({\n",
    "            \"raw_tweet\": self.X_val.X_raw,\n",
    "            \"processed_tweet\": self.X_val.X,\n",
    "            \"true_label\": self.y_val.values,\n",
    "            \"prediction\": self.results[self.best_model][\"val_pred\"]\n",
    "        })\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training embedding_mu model\n",
      "\n",
      "Epoch 1/500\n",
      "12/12 - 1s - loss: 0.6204 - accuracy: 0.6552 - val_loss: 0.5432 - val_accuracy: 0.7393\n",
      "Epoch 2/500\n",
      "12/12 - 0s - loss: 0.5037 - accuracy: 0.7722 - val_loss: 0.4963 - val_accuracy: 0.7564\n",
      "Epoch 3/500\n",
      "12/12 - 0s - loss: 0.4563 - accuracy: 0.7957 - val_loss: 0.4657 - val_accuracy: 0.7768\n",
      "Epoch 4/500\n",
      "12/12 - 0s - loss: 0.4303 - accuracy: 0.8092 - val_loss: 0.4576 - val_accuracy: 0.7774\n",
      "Epoch 5/500\n",
      "12/12 - 0s - loss: 0.4158 - accuracy: 0.8181 - val_loss: 0.4551 - val_accuracy: 0.7833\n",
      "Epoch 6/500\n",
      "12/12 - 0s - loss: 0.4059 - accuracy: 0.8228 - val_loss: 0.4550 - val_accuracy: 0.7827\n",
      "Epoch 7/500\n",
      "12/12 - 0s - loss: 0.3964 - accuracy: 0.8307 - val_loss: 0.4520 - val_accuracy: 0.7853\n",
      "Epoch 8/500\n",
      "12/12 - 0s - loss: 0.3888 - accuracy: 0.8307 - val_loss: 0.4501 - val_accuracy: 0.7892\n",
      "Epoch 9/500\n",
      "12/12 - 0s - loss: 0.3805 - accuracy: 0.8391 - val_loss: 0.4514 - val_accuracy: 0.7886\n",
      "Epoch 10/500\n",
      "12/12 - 0s - loss: 0.3732 - accuracy: 0.8415 - val_loss: 0.4521 - val_accuracy: 0.7866\n",
      "Epoch 11/500\n",
      "12/12 - 0s - loss: 0.3650 - accuracy: 0.8452 - val_loss: 0.4532 - val_accuracy: 0.7899\n",
      "Epoch 12/500\n",
      "12/12 - 0s - loss: 0.3560 - accuracy: 0.8488 - val_loss: 0.4586 - val_accuracy: 0.7840\n",
      "Epoch 13/500\n",
      "12/12 - 0s - loss: 0.3487 - accuracy: 0.8501 - val_loss: 0.4564 - val_accuracy: 0.7853\n",
      "Epoch 14/500\n",
      "12/12 - 0s - loss: 0.3401 - accuracy: 0.8568 - val_loss: 0.4625 - val_accuracy: 0.7905\n",
      "Epoch 15/500\n",
      "12/12 - 0s - loss: 0.3328 - accuracy: 0.8598 - val_loss: 0.4822 - val_accuracy: 0.7827\n",
      "Epoch 16/500\n",
      "12/12 - 0s - loss: 0.3318 - accuracy: 0.8604 - val_loss: 0.4683 - val_accuracy: 0.7925\n",
      "Epoch 17/500\n",
      "12/12 - 0s - loss: 0.3153 - accuracy: 0.8667 - val_loss: 0.4737 - val_accuracy: 0.7886\n",
      "Epoch 18/500\n",
      "12/12 - 0s - loss: 0.3050 - accuracy: 0.8745 - val_loss: 0.4785 - val_accuracy: 0.7859\n",
      "Epoch 19/500\n",
      "12/12 - 0s - loss: 0.2974 - accuracy: 0.8757 - val_loss: 0.4857 - val_accuracy: 0.7879\n",
      "Epoch 20/500\n",
      "12/12 - 0s - loss: 0.2856 - accuracy: 0.8808 - val_loss: 0.4950 - val_accuracy: 0.7886\n",
      "Epoch 21/500\n",
      "12/12 - 0s - loss: 0.2795 - accuracy: 0.8852 - val_loss: 0.5033 - val_accuracy: 0.7853\n",
      "Epoch 22/500\n",
      "12/12 - 0s - loss: 0.2731 - accuracy: 0.8892 - val_loss: 0.5150 - val_accuracy: 0.7859\n",
      "Epoch 23/500\n",
      "12/12 - 0s - loss: 0.2594 - accuracy: 0.8931 - val_loss: 0.5155 - val_accuracy: 0.7859\n",
      "Epoch 24/500\n",
      "12/12 - 0s - loss: 0.2527 - accuracy: 0.8984 - val_loss: 0.5323 - val_accuracy: 0.7873\n",
      "Epoch 25/500\n",
      "12/12 - 0s - loss: 0.2397 - accuracy: 0.9020 - val_loss: 0.5472 - val_accuracy: 0.7748\n",
      "Epoch 26/500\n",
      "12/12 - 0s - loss: 0.2386 - accuracy: 0.9023 - val_loss: 0.5675 - val_accuracy: 0.7840\n",
      "\n",
      "Training embedding_minmax model\n",
      "\n",
      "Epoch 1/500\n",
      "12/12 - 0s - loss: 0.6981 - accuracy: 0.5742 - val_loss: 0.6413 - val_accuracy: 0.5949\n",
      "Epoch 2/500\n",
      "12/12 - 0s - loss: 0.6091 - accuracy: 0.6826 - val_loss: 0.5742 - val_accuracy: 0.7065\n",
      "Epoch 3/500\n",
      "12/12 - 0s - loss: 0.5501 - accuracy: 0.7371 - val_loss: 0.5230 - val_accuracy: 0.7603\n",
      "Epoch 4/500\n",
      "12/12 - 0s - loss: 0.5096 - accuracy: 0.7654 - val_loss: 0.5021 - val_accuracy: 0.7538\n",
      "Epoch 5/500\n",
      "12/12 - 0s - loss: 0.4787 - accuracy: 0.7831 - val_loss: 0.4835 - val_accuracy: 0.7715\n",
      "Epoch 6/500\n",
      "12/12 - 0s - loss: 0.4640 - accuracy: 0.7882 - val_loss: 0.4892 - val_accuracy: 0.7498\n",
      "Epoch 7/500\n",
      "12/12 - 0s - loss: 0.4501 - accuracy: 0.7959 - val_loss: 0.4721 - val_accuracy: 0.7702\n",
      "Epoch 8/500\n",
      "12/12 - 0s - loss: 0.4366 - accuracy: 0.8051 - val_loss: 0.4675 - val_accuracy: 0.7735\n",
      "Epoch 9/500\n",
      "12/12 - 0s - loss: 0.4394 - accuracy: 0.8064 - val_loss: 0.4805 - val_accuracy: 0.7557\n",
      "Epoch 10/500\n",
      "12/12 - 0s - loss: 0.4296 - accuracy: 0.8090 - val_loss: 0.4817 - val_accuracy: 0.7735\n",
      "Epoch 11/500\n",
      "12/12 - 0s - loss: 0.4270 - accuracy: 0.8095 - val_loss: 0.4764 - val_accuracy: 0.7597\n",
      "Epoch 12/500\n",
      "12/12 - 0s - loss: 0.4189 - accuracy: 0.8184 - val_loss: 0.4628 - val_accuracy: 0.7643\n",
      "Epoch 13/500\n",
      "12/12 - 0s - loss: 0.4128 - accuracy: 0.8167 - val_loss: 0.4962 - val_accuracy: 0.7544\n",
      "Epoch 14/500\n",
      "12/12 - 0s - loss: 0.4235 - accuracy: 0.8158 - val_loss: 0.4651 - val_accuracy: 0.7728\n",
      "Epoch 15/500\n",
      "12/12 - 0s - loss: 0.4061 - accuracy: 0.8259 - val_loss: 0.4664 - val_accuracy: 0.7715\n",
      "Epoch 16/500\n",
      "12/12 - 0s - loss: 0.4092 - accuracy: 0.8225 - val_loss: 0.4678 - val_accuracy: 0.7787\n",
      "Epoch 17/500\n",
      "12/12 - 0s - loss: 0.4024 - accuracy: 0.8263 - val_loss: 0.4806 - val_accuracy: 0.7794\n",
      "Epoch 18/500\n",
      "12/12 - 0s - loss: 0.4206 - accuracy: 0.8131 - val_loss: 0.4908 - val_accuracy: 0.7557\n",
      "Epoch 19/500\n",
      "12/12 - 0s - loss: 0.4021 - accuracy: 0.8223 - val_loss: 0.4690 - val_accuracy: 0.7682\n",
      "Epoch 20/500\n",
      "12/12 - 0s - loss: 0.3948 - accuracy: 0.8302 - val_loss: 0.4665 - val_accuracy: 0.7722\n",
      "Epoch 21/500\n",
      "12/12 - 0s - loss: 0.3900 - accuracy: 0.8340 - val_loss: 0.4732 - val_accuracy: 0.7676\n",
      "Epoch 22/500\n",
      "12/12 - 0s - loss: 0.3854 - accuracy: 0.8340 - val_loss: 0.4697 - val_accuracy: 0.7748\n",
      "Epoch 23/500\n",
      "12/12 - 0s - loss: 0.3882 - accuracy: 0.8319 - val_loss: 0.4747 - val_accuracy: 0.7722\n",
      "Epoch 24/500\n",
      "12/12 - 0s - loss: 0.3914 - accuracy: 0.8261 - val_loss: 0.4688 - val_accuracy: 0.7735\n",
      "Epoch 25/500\n",
      "12/12 - 0s - loss: 0.3841 - accuracy: 0.8348 - val_loss: 0.4773 - val_accuracy: 0.7630\n",
      "Epoch 26/500\n",
      "12/12 - 0s - loss: 0.3786 - accuracy: 0.8356 - val_loss: 0.4725 - val_accuracy: 0.7748\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf_nn = TfModel(\n",
    "    train_we, train_labels, validation_we, validation_labels, epochs=500, patience=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best word embeddings scheme: embedding_mu\n",
      "Validation f1 score = 0.789\n"
     ]
    }
   ],
   "source": [
    "print(f\"Best word embeddings scheme: {tf_nn.best_model}\\nValidation f1 score = {tf_nn.best_score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall we see that applying word embeddings and classifying with the Neural Network does not hugely improve performance over the baseline model. Not shown in the code above, we also experimented with removing common stop words from the corpus by applying IDF to all the words in the training tweets, and removing words which were very common across all (disaster and non-disaster) tweets. This did not improve model performance and generally degraded it.\n",
    "\n",
    "As with the other models we inspect some false positives/negatives to look for patterns in the mis-classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_positives = tf_nn.inspect_val[\n",
    "    (tf_nn.inspect_val[\"true_label\"] == 0) & \n",
    "    (tf_nn.inspect_val[\"prediction\"] == 1)\n",
    "]\n",
    "false_negatives = tf_nn.inspect_val[\n",
    "    (tf_nn.inspect_val[\"true_label\"] == 1) & \n",
    "    (tf_nn.inspect_val[\"prediction\"] == 0)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False Positives\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_tweet</th>\n",
       "      <th>processed_tweet</th>\n",
       "      <th>true_label</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>The Five Fatal Flaws in the Iran Deal https://t.co/ztfEAd8GId via @YouTube</td>\n",
       "      <td>the five fatal flaws in the iran deal &lt;url&gt; via &lt;user&gt;</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Camping in a war zone with roving raccoons toughens city slicker http://t.co/oJuS08yZrq</td>\n",
       "      <td>camping in a war zone with roving raccoons toughens city slicker &lt;url&gt;</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>Nuclear deal disaster.\\r\\n\\r\\n#IranDeal #NoNuclearIran #BadIranDeal @JebBush @BarackObama http://t.co/z7phPjtqud</td>\n",
       "      <td>nuclear deal disaster &lt;hashtag&gt; iran deal &lt;hashtag&gt; no nuclear iran &lt;hashtag&gt; bad iran deal &lt;user&gt; &lt;user&gt; &lt;url&gt;</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>#Sismo M 1.3 - 1km NNE of The Geysers California: Time2015-08-05 23:40:21 UTC2015-08-05 16:40:21 -07:00 a... http://t.co/x6el3ySYcn #CS</td>\n",
       "      <td>&lt;hashtag&gt; sismo m &lt;number&gt; &lt;number&gt; km nne &lt;allcaps&gt; of the geysers california time &lt;number&gt; &lt;number&gt; &lt;number&gt; &lt;number&gt; utc &lt;allcaps&gt; &lt;number&gt; &lt;number&gt; &lt;number&gt; &lt;number&gt; &lt;number&gt; a &lt;repeat&gt; &lt;url&gt; &lt;hashtag&gt; cs &lt;allcaps&gt;</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>Did you miss the #BitCoin explosion - Don't miss out - #Hangout tonight at 8:30PM EST ===&amp;gt;&amp;gt;&amp;gt; http://t.co/qKaHXwLWXa</td>\n",
       "      <td>did you miss the &lt;hashtag&gt; bit coin explosion don t miss out &lt;hashtag&gt; hangout tonight at &lt;number&gt; pm &lt;allcaps&gt; est gt gt gt &lt;url&gt;</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>9 Charts Prove Financial Crisis Part 2 Has BEGUN!: The Financial Armageddon Economic Collapse Blog tracks tren... http://t.co/vHCXTvCINr</td>\n",
       "      <td>&lt;number&gt; charts prove financial crisis part &lt;number&gt; has begun &lt;allcaps&gt; the financial armageddon economic collapse blog tracks tren &lt;repeat&gt; &lt;url&gt;</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>Meltdown</td>\n",
       "      <td>meltdown</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>@freefromwolves GodsLove &amp;amp; #thankU brother Danny for RT of NEW VIDEO http://t.co/cybKsXHF7d The Coming Apocalyptic US Earthquake &amp;amp; Tsunami</td>\n",
       "      <td>&lt;user&gt; godslove amp &lt;hashtag&gt; thank u brother danny for rt &lt;allcaps&gt; of new &lt;allcaps&gt; video &lt;url&gt; the coming apocalyptic us &lt;allcaps&gt; earthquake amp tsunami</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>she keep it wet like tsunami.</td>\n",
       "      <td>she keep it wet like tsunami</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>Violent Forces Radio: Now Playing Agony - Storm of the apocalypse\\r\\nTuneIn Player @ http://t.co/XsSgEdSbH4</td>\n",
       "      <td>violent forces radio now playing agony storm of the apocalypse tunein player &lt;url&gt;</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                              raw_tweet  \\\n",
       "11                                                                           The Five Fatal Flaws in the Iran Deal https://t.co/ztfEAd8GId via @YouTube   \n",
       "48                                                              Camping in a war zone with roving raccoons toughens city slicker http://t.co/oJuS08yZrq   \n",
       "74                                     Nuclear deal disaster.\\r\\n\\r\\n#IranDeal #NoNuclearIran #BadIranDeal @JebBush @BarackObama http://t.co/z7phPjtqud   \n",
       "76              #Sismo M 1.3 - 1km NNE of The Geysers California: Time2015-08-05 23:40:21 UTC2015-08-05 16:40:21 -07:00 a... http://t.co/x6el3ySYcn #CS   \n",
       "80                         Did you miss the #BitCoin explosion - Don't miss out - #Hangout tonight at 8:30PM EST ===&gt;&gt;&gt; http://t.co/qKaHXwLWXa   \n",
       "101            9 Charts Prove Financial Crisis Part 2 Has BEGUN!: The Financial Armageddon Economic Collapse Blog tracks tren... http://t.co/vHCXTvCINr   \n",
       "128                                                                                                                                            Meltdown   \n",
       "132  @freefromwolves GodsLove &amp; #thankU brother Danny for RT of NEW VIDEO http://t.co/cybKsXHF7d The Coming Apocalyptic US Earthquake &amp; Tsunami   \n",
       "137                                                                                                                       she keep it wet like tsunami.   \n",
       "139                                         Violent Forces Radio: Now Playing Agony - Storm of the apocalypse\\r\\nTuneIn Player @ http://t.co/XsSgEdSbH4   \n",
       "\n",
       "                                                                                                                                                                                                                processed_tweet  \\\n",
       "11                                                                                                                                                                       the five fatal flaws in the iran deal <url> via <user>   \n",
       "48                                                                                                                                                       camping in a war zone with roving raccoons toughens city slicker <url>   \n",
       "74                                                                                                              nuclear deal disaster <hashtag> iran deal <hashtag> no nuclear iran <hashtag> bad iran deal <user> <user> <url>   \n",
       "76   <hashtag> sismo m <number> <number> km nne <allcaps> of the geysers california time <number> <number> <number> <number> utc <allcaps> <number> <number> <number> <number> <number> a <repeat> <url> <hashtag> cs <allcaps>   \n",
       "80                                                                                           did you miss the <hashtag> bit coin explosion don t miss out <hashtag> hangout tonight at <number> pm <allcaps> est gt gt gt <url>   \n",
       "101                                                                         <number> charts prove financial crisis part <number> has begun <allcaps> the financial armageddon economic collapse blog tracks tren <repeat> <url>   \n",
       "128                                                                                                                                                                                                                    meltdown   \n",
       "132                                                                <user> godslove amp <hashtag> thank u brother danny for rt <allcaps> of new <allcaps> video <url> the coming apocalyptic us <allcaps> earthquake amp tsunami   \n",
       "137                                                                                                                                                                                                she keep it wet like tsunami   \n",
       "139                                                                                                                                          violent forces radio now playing agony storm of the apocalypse tunein player <url>   \n",
       "\n",
       "     true_label  prediction  \n",
       "11            0           1  \n",
       "48            0           1  \n",
       "74            0           1  \n",
       "76            0           1  \n",
       "80            0           1  \n",
       "101           0           1  \n",
       "128           0           1  \n",
       "132           0           1  \n",
       "137           0           1  \n",
       "139           0           1  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"False Positives\")\n",
    "false_positives.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of the false positives contain words that might be expected to confuse the classifier, e.g. `explosion, blast, snowstorm, armaggeddon, collapse` etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False Negatives\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_tweet</th>\n",
       "      <th>processed_tweet</th>\n",
       "      <th>true_label</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Burned dog finds new home with young burn victim http://t.co/Pqrjvgvgxg</td>\n",
       "      <td>burned dog finds new home with young burn victim &lt;url&gt;</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>@PyramidHead76 one good thing came out of watching the film.  Was too traumatised to watch show so started Halt &amp;amp; Catch Fire on Amazon. :D</td>\n",
       "      <td>&lt;user&gt; one good thing came out of watching the film was too traumatised to watch show so started halt amp catch fire on amazon &lt;smile&gt;</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>@MrMikeEaton @Muazimus_Prime hill hill mountain volcano of hell mountain hill hil.</td>\n",
       "      <td>&lt;user&gt; &lt;user&gt; hill hill mountain volcano of hell mountain hill hil</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Gonna call up tomorrow with the aul 'emergency dental appointment' excuse just like the whole tooth falling out incident of last year</td>\n",
       "      <td>gonna call up tomorrow with the aul emergency dental appointment excuse just like the whole tooth falling out incident of last year</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>@TheEvilOlives It's the closest structure to the hypo centre that wasn't completely obliterated.</td>\n",
       "      <td>&lt;user&gt; it s the closest structure to the hypo centre that wasn t completely obliterated</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>Bigamist and his 'first' wife are charged in the deaths of his 'second' pregnant wife her child 8 her mothe... http://t.co/rTEuGB5Tnv</td>\n",
       "      <td>bigamist and his first wife are charged in the deaths of his second pregnant wife her child &lt;number&gt; her mothe &lt;repeat&gt; &lt;url&gt;</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>And so it begins.. day one of the snow apocalypse</td>\n",
       "      <td>and so it begins &lt;repeat&gt; day one of the snow apocalypse</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>@Hurricane_Dolce no prob</td>\n",
       "      <td>&lt;user&gt; no prob</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>Zayn Malik &amp;amp; Perrie Edwards End Engagement: Shes Devastated http://t.co/GedOxSPpL9 http://t.co/ACZRUOrYtD</td>\n",
       "      <td>zayn malik amp perrie edwards end engagement shes devastated &lt;url&gt; &lt;url&gt;</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>We're #hiring! Click to apply: RN II/EMERGENCY SERVICES/FT/7P-7A - http://t.co/NV3Uxv9IMX #Nursing #Houston TX http://t.co/ej30IhrEA9</td>\n",
       "      <td>we re &lt;hashtag&gt; hiring click to apply rn &lt;allcaps&gt; ii emergency &lt;allcaps&gt; services ft &lt;allcaps&gt; &lt;number&gt; p &lt;number&gt; a &lt;url&gt; &lt;hashtag&gt; nursing &lt;hashtag&gt; houston tx &lt;allcaps&gt; &lt;url&gt;</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                         raw_tweet  \\\n",
       "1                                                                          Burned dog finds new home with young burn victim http://t.co/Pqrjvgvgxg   \n",
       "5   @PyramidHead76 one good thing came out of watching the film.  Was too traumatised to watch show so started Halt &amp; Catch Fire on Amazon. :D   \n",
       "9                                                               @MrMikeEaton @Muazimus_Prime hill hill mountain volcano of hell mountain hill hil.   \n",
       "22           Gonna call up tomorrow with the aul 'emergency dental appointment' excuse just like the whole tooth falling out incident of last year   \n",
       "38                                                @TheEvilOlives It's the closest structure to the hypo centre that wasn't completely obliterated.   \n",
       "72           Bigamist and his 'first' wife are charged in the deaths of his 'second' pregnant wife her child 8 her mothe... http://t.co/rTEuGB5Tnv   \n",
       "81                                                                                               And so it begins.. day one of the snow apocalypse   \n",
       "83                                                                                                                        @Hurricane_Dolce no prob   \n",
       "86                          Zayn Malik &amp; Perrie Edwards End Engagement: Shes Devastated http://t.co/GedOxSPpL9 http://t.co/ACZRUOrYtD   \n",
       "94           We're #hiring! Click to apply: RN II/EMERGENCY SERVICES/FT/7P-7A - http://t.co/NV3Uxv9IMX #Nursing #Houston TX http://t.co/ej30IhrEA9   \n",
       "\n",
       "                                                                                                                                                                       processed_tweet  \\\n",
       "1                                                                                                                               burned dog finds new home with young burn victim <url>   \n",
       "5                                               <user> one good thing came out of watching the film was too traumatised to watch show so started halt amp catch fire on amazon <smile>   \n",
       "9                                                                                                                   <user> <user> hill hill mountain volcano of hell mountain hill hil   \n",
       "22                                                 gonna call up tomorrow with the aul emergency dental appointment excuse just like the whole tooth falling out incident of last year   \n",
       "38                                                                                             <user> it s the closest structure to the hypo centre that wasn t completely obliterated   \n",
       "72                                                       bigamist and his first wife are charged in the deaths of his second pregnant wife her child <number> her mothe <repeat> <url>   \n",
       "81                                                                                                                            and so it begins <repeat> day one of the snow apocalypse   \n",
       "83                                                                                                                                                                      <user> no prob   \n",
       "86                                                                                                   zayn malik amp perrie edwards end engagement shes devastated <url> <url>   \n",
       "94  we re <hashtag> hiring click to apply rn <allcaps> ii emergency <allcaps> services ft <allcaps> <number> p <number> a <url> <hashtag> nursing <hashtag> houston tx <allcaps> <url>   \n",
       "\n",
       "    true_label  prediction  \n",
       "1            1           0  \n",
       "5            1           0  \n",
       "9            1           0  \n",
       "22           1           0  \n",
       "38           1           0  \n",
       "72           1           0  \n",
       "81           1           0  \n",
       "83           1           0  \n",
       "86           1           0  \n",
       "94           1           0  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"False Negatives\")\n",
    "false_negatives.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again it seems like one of the main issues is that some of the tweets in the training data are mislabeled, e.g. `103: Skinny Jeans are Hazardous for Your Health! ...` and `94: We're #hiring! ...` do not seem like disaster tweets. To improve performance significantly we may need to get hold of a correctly labeled dataset.\n",
    "\n",
    "### Assessing using the test data set\n",
    "\n",
    "As before, we apply the preprocessing steps and use the trained model to predict labels on the test data, and submit to Kaggle to get the final score of model performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8788 processed input words, 8010 found in embeddings (91.15%).\n",
      "Score on Kaggle = 0.80907\n"
     ]
    }
   ],
   "source": [
    "test_we = WordEmbeddings(test[\"text\"], glove_twitter)\n",
    "X_test_df = getattr(test_we, tf_nn.best_model)\n",
    "\n",
    "# Get the best performing model, and use it to predict the test data:\n",
    "model = tf_nn.results[tf_nn.best_model][\"model\"]\n",
    "proba_model = tf.keras.Sequential([\n",
    "    model, \n",
    "    tf.keras.layers.Softmax()\n",
    "])\n",
    "test_proba_pred = proba_model.predict(X_test_df)\n",
    "test_pred = np.argmax(test_proba_pred, axis=1)\n",
    "\n",
    "nn_submission = pd.DataFrame({\"id\": test[\"id\"], \"target\": test_pred})\n",
    "nn_submission.to_csv(\"neural_network_test_predictions.csv\", index=False)\n",
    "\n",
    "print(\"Score on Kaggle = 0.80907\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "After training all 4 models and submitting their test data predictions on Kaggle we get the following overall scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>F1-Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Baseline Bernoulli Naive Bayes</td>\n",
       "      <td>0.79282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Optimized Naive Bayes</td>\n",
       "      <td>0.79313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.78945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Neural Network</td>\n",
       "      <td>0.80907</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Model  F1-Score\n",
       "0  Baseline Bernoulli Naive Bayes   0.79282\n",
       "1           Optimized Naive Bayes   0.79313\n",
       "2             Logistic Regression   0.78945\n",
       "3                  Neural Network   0.80907"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({\n",
    "    \"Model\": [ \n",
    "        \"Baseline Bernoulli Naive Bayes\", \n",
    "        \"Optimized Naive Bayes\", \n",
    "        \"Logistic Regression\", \n",
    "        \"Neural Network\"],\n",
    "    \"F1-Score\": [0.79282, 0.79313, 0.78945, 0.80907]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are able to achieve modest improvements on the baseline model with both the Neural Network and the Optimized Naive Bayes\tmodels. The Logistic Regression model achieves slightly worse performance. Overall the main problem we identified with all the models in the error analyses was that some of the training data appears to be mislabeled, which seems to have the effect of confusing the classifiers and degrading performance. If we wanted to counteract this we could seek new training data or perhaps manually reclassify the suspected mislabeled training examples. It was interesting to note that using bigram features instead of unigram features degraded the logistic regression model's performance, when intuitively we expected it may improve it. Out hypothesis is that this is because it made the feature space too large relative to the number of training samples, and so the data became too sparse to accurately estimate model coefficients.\n",
    "\n",
    "In practice if we were deploying this model as a production application we would likely use either the baseline or optimized Naive Bayes models, since computationally they are much less complex than the Neural Network pipeline, which involves a lengthy training process of loading a large embeddings scheme into memory and then applying expensive computations to embed each training example. The overall performance improvement in using this complex architecture was fairly negligible."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
